{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing, linear_model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split, ShuffleSplit, KFold, cross_val_score, GridSearchCV, StratifiedKFold\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import root_mean_squared_error, r2_score,  mean_squared_error\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import SimpleImputer, KNNImputer, IterativeImputer\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_name in os.listdir(\"files\"):\n",
    "    if file_name.endswith('.csv'):\n",
    "        file_path = os.path.join(\"files\", file_name)\n",
    "\n",
    "        df_name = os.path.splitext(file_name)[0]\n",
    "        globals()[df_name] = pd.read_csv(file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simplification of categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge sports data and extract sports categories\n",
    "def extract_sports_category(dataset, sports_code):\n",
    "    merged = pd.merge(dataset, sports_code, left_on=\"Sports\", right_on=\"Code\")\n",
    "    merged[\"Sports_Category\"] = merged[\"Categorie\"]\n",
    "    return merged[[\"PRIMARY_KEY\", \"Sports_Category\"]]\n",
    "\n",
    "learn_sports = extract_sports_category(learn_dataset_sport, code_Sports)\n",
    "test_sports = extract_sports_category(test_dataset_sport, code_Sports)\n",
    "\n",
    "# Merge departments into regions and extract relevant region columns\n",
    "def merge_and_extract_region(df, merge_column, region_column_name):\n",
    "    merged = pd.merge(df, departments, left_on=merge_column, right_on=\"DEP\")\n",
    "    merged[region_column_name] = merged[\"REG\"]\n",
    "    return merged.drop([\"Nom du département\", \"REG\", \"DEP\", merge_column], axis=1)\n",
    "\n",
    "learn_dataset_job = merge_and_extract_region(learn_dataset_job, \"JOB_DEP\", \"REG_JOB\")\n",
    "learn_dataset_retired_jobs = merge_and_extract_region(learn_dataset_retired_jobs, \"JOB_DEP\", \"REG_JOB\")\n",
    "learn_dataset_retired_jobs = merge_and_extract_region(learn_dataset_retired_jobs, \"FORMER_DEP\", \"REG_FORMER\")\n",
    "\n",
    "test_dataset_job = merge_and_extract_region(test_dataset_job, \"JOB_DEP\", \"REG_JOB\")\n",
    "test_dataset_retired_jobs = merge_and_extract_region(test_dataset_retired_jobs, \"JOB_DEP\", \"REG_JOB\")\n",
    "test_dataset_retired_jobs = merge_and_extract_region(test_dataset_retired_jobs, \"FORMER_DEP\", \"REG_FORMER\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Economic sector into fewer categories (and numeric instead of object/string)\n",
    "def sector_mapping(nace_code):\n",
    "    if nace_code == \"AZ\":  \n",
    "        return \"Agriculture, forestry and fishing)\"\n",
    "    elif \"BZ\" <= nace_code <= \"EZ\":\n",
    "        return \"Manufacturing, mining and quarrying and other industrial activities\"\n",
    "    elif nace_code == \"FZ\": \n",
    "        return \"Construction\"\n",
    "    elif \"GZ\" <= nace_code <= \"IZ\":  \n",
    "        return \"Wholesale and retail trade, transportation and storage, accommodation and food service activities\"\n",
    "    elif \"JA\" <= nace_code <= \"JC\":\n",
    "        return \"Information and communication\"\n",
    "    elif nace_code == \"KZ\": \n",
    "        return \"Financial and insurance activities\"\n",
    "    elif nace_code == \"LZ\": \n",
    "        return \"Real estate activities\"\n",
    "    elif \"MA\" <= nace_code <= \"NZ\":\n",
    "        return \"Professional, scientific, technical, administrative and support service activities\"\n",
    "    elif \"OZ\" <= nace_code <= \"QB\":\n",
    "        return \"Public administration and defence, education, human health and social work activities\"\n",
    "    elif \"RZ\" <= nace_code <= \"UZ\":\n",
    "        return \"Other services activities\"\n",
    "    else:\n",
    "        return \"Unknown Sector\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_Economic_sector[\"Nomenclature\"] = code_Economic_sector[\"Code\"].map(sector_mapping)\n",
    "code_Economic_sector[\"Economic_sector_num\"] = pd.factorize(code_Economic_sector[\"Nomenclature\"])[0] + 1\n",
    "\n",
    "code_HIGHEST_CREDENTIAL[\"HIGHEST_CREDENTIAL_num\"] = pd.factorize(code_HIGHEST_CREDENTIAL[\"Code\"])[0] + 1\n",
    "code_act[\"act_num\"] = pd.factorize(code_act[\"Code\"])[0] + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_data = learn_dataset\n",
    "\n",
    "learn_data = pd.merge(learn_data, code_act, left_on=\"act\", right_on=\"Code\", how=\"left\")\n",
    "learn_data.drop([\"Code\", \"Libellé\"], axis=1, inplace=True)\n",
    "learn_data = pd.merge(learn_data, code_HIGHEST_CREDENTIAL, left_on=\"HIGHEST_CREDENTIAL\", right_on=\"Code\", how=\"left\")\n",
    "learn_data.drop([\"Code\", \"Libellé\", \"HIGHEST_CREDENTIAL\"], axis=1, inplace=True)\n",
    "\n",
    "#for imputation fitting\n",
    "learn_data = pd.merge(learn_data, city_pop, on=\"INSEE_CODE\", how=\"left\")\n",
    "learn_data = pd.merge(learn_data, city_loc, on=\"INSEE_CODE\", how=\"left\")\n",
    "\n",
    "\n",
    "test_data = test_dataset\n",
    "\n",
    "test_data = pd.merge(test_data, code_act, left_on=\"act\", right_on=\"Code\", how=\"left\")\n",
    "test_data.drop([\"Code\", \"Libellé\"], axis=1, inplace=True)\n",
    "test_data = pd.merge(test_data, code_HIGHEST_CREDENTIAL, left_on=\"HIGHEST_CREDENTIAL\", right_on=\"Code\", how=\"left\")\n",
    "test_data.drop([\"Code\", \"Libellé\", \"HIGHEST_CREDENTIAL\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_dfs = [learn_dataset_emp_contract, learn_dataset_job, learn_dataset_retired_former, learn_dataset_retired_jobs, learn_dataset_retired_pension, learn_sports]\n",
    "test_dfs = [test_dataset_emp_contract, test_dataset_job, test_dataset_retired_former, test_dataset_retired_jobs, test_dataset_retired_pension, test_sports]\n",
    "\n",
    "for df in learn_dfs:\n",
    "    learn_data = pd.merge(learn_data, df, on=\"PRIMARY_KEY\", how=\"outer\")\n",
    "\n",
    "for df in test_dfs:\n",
    "    test_data = pd.merge(test_data, df, on=\"PRIMARY_KEY\", how=\"outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to combine columns ending with `_x` and `_y` into a single base column\n",
    "def combine_duplicate_columns(dataframe):\n",
    "    for column in dataframe.columns:\n",
    "        if column.endswith('_x'):\n",
    "            base_column = column[:-2]  # Remove `_x` suffix\n",
    "            y_column = base_column + '_y'\n",
    "            if y_column in dataframe.columns:\n",
    "                # Combine the `_x` and `_y` columns\n",
    "                dataframe[base_column] = dataframe[column].fillna(dataframe[y_column])\n",
    "                # Drop the original `_x` and `_y` columns\n",
    "                dataframe.drop(columns=[column, y_column], inplace=True)\n",
    "    return dataframe\n",
    "\n",
    "# Apply the function to both datasets\n",
    "learn_data = combine_duplicate_columns(learn_data)\n",
    "test_data = combine_duplicate_columns(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formatting - for linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "def household_num(value):\n",
    "    parts = value.split('|')  # Split the value by '|'\n",
    "    if parts[1] in {'1', '2', '3'}:  # For M|1|-- to M|3|--\n",
    "        return int(parts[1])\n",
    "    elif parts[1] == '4':  # For M|4|1 to M|4|4\n",
    "        return 4 + (int(parts[2]) - 1)  # 4 + (1-1), 4 + (2-1), etc.\n",
    "    return None  # Handle unexpected cases gracefully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_HOUSEHOLD_TYPE['HOUSEHOLD_TYPE_num'] = code_HOUSEHOLD_TYPE['Code'].apply(household_num)\n",
    "learn_data['HOUSEHOLD_TYPE'] = learn_data['HOUSEHOLD_TYPE'].apply(household_num)\n",
    "test_data['HOUSEHOLD_TYPE'] = test_data['HOUSEHOLD_TYPE'].apply(household_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_columns(primary_col, fallback_col):\n",
    "    \"\"\"Combine two columns, filling missing values in the primary column with values from the fallback column.\"\"\"\n",
    "    return primary_col.fillna(fallback_col) if fallback_col is not None else primary_col\n",
    "\n",
    "def preprocess_employee_data(data, economic_sector_code, work_description_map):\n",
    "    # Extract numeric values from specific string columns\n",
    "    data[\"employee_count\"] = data[\"employee_count\"].str.extract(r'tr_(\\d)')[0].astype(\"Int64\")\n",
    "    data[\"Employer_category\"] = data[\"Employer_category\"].str.extract(r'ct_(\\d)')[0].astype(\"Int64\")\n",
    "    \n",
    "    # Merge with economic sector codes\n",
    "    data = data.merge(economic_sector_code, left_on=\"Economic_sector\", right_on=\"Code\", how=\"left\")\n",
    "    \n",
    "    # Merge with work description map and clean up columns\n",
    "    data = data.merge(work_description_map, left_on=\"work_description\", right_on=\"N3\", how=\"left\")\n",
    "    data.drop([\"work_description\", \"N3\", \"N2\"], axis=1, inplace=True)\n",
    "    data[\"work_description\"] = data[\"N1\"].str.extract(r'csp_(\\d)')[0].astype(\"Int64\")\n",
    "    data.drop(\"N1\", axis=1, inplace=True)\n",
    "    \n",
    "    # Combine relevant columns for contracts and pay\n",
    "    data[\"emp_contract\"] = combine_columns(data[\"emp_contract\"], data[\"former_emp_contract\"])\n",
    "    data[\"Pay\"] = combine_columns(data[\"Pay\"], data[\"RETIREMENT_PAY\"])\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Apply preprocessing to both learn and test datasets\n",
    "learn_data = preprocess_employee_data(learn_data, code_Economic_sector, code_work_description_map)\n",
    "test_data = preprocess_employee_data(test_data, code_Economic_sector, code_work_description_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_data[\"sex\"] = pd.factorize(learn_data[\"sex\"])[0]\n",
    "learn_data[\"studying\"] = learn_data[\"studying\"].astype(\"int64\")\n",
    "learn_data[\"Sports_Category\"] = learn_data[\"Sports_Category\"].fillna(0).astype(\"int64\")\n",
    "learn_data[\"REG_JOB\"] = pd.to_numeric(learn_data[\"REG_JOB\"], errors='coerce').astype('Int64')\n",
    "learn_data[\"REG_FORMER\"] = pd.to_numeric(learn_data[\"REG_FORMER\"], errors='coerce').astype('Int64')\n",
    "learn_data[\"retirement_age\"] = pd.to_numeric(learn_data[\"retirement_age\"], errors='coerce').astype('Int64')\n",
    "learn_data[\"WORKING_HOURS\"] = pd.to_numeric(learn_data[\"WORKING_HOURS\"], errors='coerce').astype('Int64')\n",
    "learn_data[\"Economic_sector_num\"] = pd.to_numeric(learn_data[\"Economic_sector_num\"], errors='coerce').astype('Int64')\n",
    "learn_data[\"Pay\"] = pd.to_numeric(learn_data[\"Pay\"], errors='coerce').astype('Int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_na_with_category(column_name):\n",
    "    global learn_data  # Ensures we modify the global learn_data directly\n",
    "\n",
    "    # Convert the column to categorical\n",
    "    learn_data[column_name] = learn_data[column_name].astype('category')\n",
    "    \n",
    "    # Define categories to add\n",
    "    additional_categories = ['Unemployed', 'Retired_Missing', 'Employed_Missing']\n",
    "    \n",
    "    # Add the specified categories\n",
    "    learn_data[column_name] = learn_data[column_name].cat.add_categories(additional_categories)\n",
    "    \n",
    "    learn_data.loc[(learn_data[column_name].isna()) & (learn_data['JOB_42'].astype(str).str.startswith('csp_7')), column_name] = 'Retired_Missing'\n",
    "    learn_data.loc[(learn_data[column_name].isna()) & (learn_data['act_num'] == 1), column_name] = 'Employed_Missing'\n",
    "    learn_data.loc[(learn_data[column_name].isna()) & ((learn_data['JOB_42'].astype(str).str.startswith('csp_8')) | (learn_data['act_num'] == 2)), column_name] = 'Unemployed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_na_with_category(\"emp_contract\")\n",
    "replace_na_with_category(\"TYPE_OF_CONTRACT\")\n",
    "replace_na_with_category(\"WORK_CONDITION\")\n",
    "replace_na_with_category(\"labor_force_status\")\n",
    "replace_na_with_category(\"Economic_sector_num\")\n",
    "replace_na_with_category(\"REG_JOB\")\n",
    "replace_na_with_category(\"REG_FORMER\")\n",
    "replace_na_with_category(\"work_description\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_data['Employer_category'] = learn_data.apply(\n",
    "    lambda row: 10 if pd.isna(row['Employer_category']) and \n",
    "                        (str(row['JOB_42']).startswith('csp_8') or row['act_num'] == 2)  # unemployed\n",
    "                else (0 if pd.isna(row['Employer_category']) else row['Employer_category']),  # employed and retired missing\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "learn_data['employee_count'] = learn_data.apply(\n",
    "    lambda row: 7 if pd.isna(row['employee_count']) and \n",
    "                        (str(row['JOB_42']).startswith('csp_8') or row['act_num'] == 2)  # unemployed\n",
    "                else (0 if pd.isna(row['employee_count']) else row['employee_count']),\n",
    "    axis=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_data.loc[learn_data['JOB_42'].astype(str).str.startswith('csp_7'), 'JOB_42'] = learn_data['FORMER_JOB_42']\n",
    "learn_data.loc[(learn_data['emp_contract'] == 'Unemployed') & (learn_data['Pay'].isna()), 'Pay'] = 0\n",
    "learn_data.loc[(learn_data['emp_contract'] == 'Unemployed') & (learn_data['WORKING_HOURS'].isna()), 'WORKING_HOURS'] = 0\n",
    "\n",
    "learn_data = learn_data.drop(columns=[\"act\", \"former_emp_contract\", \"RETIREMENT_PAY\", \"retirement_age\", \"Economic_sector\", \"Code\", \"Libellé\", \"Nomenclature\", \"INSEE_CODE\", \"X\", \"Y\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define job-specific pay and working hours\n",
    "job_defaults = {\n",
    "    'csp_1': {'Pay': 50000, 'WORKING_HOURS': 2860},  # Agriculture\n",
    "    'csp_2_1': {'Pay': 24000, 'WORKING_HOURS': 2288},  # Artisans\n",
    "    'csp_2_2': {'Pay': 39937, 'WORKING_HOURS': 2444},  # Commerçant\n",
    "    'csp_2_3': {'Pay': 58248, 'WORKING_HOURS': 2704},  # Chefs d'enterprise\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to fill missing values for Pay and WORKING_HOURS\n",
    "def fill_job_defaults(data, job_defaults):\n",
    "    for job, defaults in job_defaults.items():\n",
    "        # Handle string startswith for specific cases (e.g., 'csp_1')\n",
    "        job_condition = data['JOB_42'].astype(str).str.startswith(job) if '_' not in job else (data['JOB_42'] == job)\n",
    "        \n",
    "        # Fill missing Pay and WORKING_HOURS\n",
    "        data.loc[job_condition & data['Pay'].isna(), 'Pay'] = defaults['Pay']\n",
    "        data.loc[job_condition & data['WORKING_HOURS'].isna(), 'WORKING_HOURS'] = defaults['WORKING_HOURS']\n",
    "\n",
    "# Apply the function to both datasets\n",
    "fill_job_defaults(learn_data, job_defaults)\n",
    "fill_job_defaults(test_data, job_defaults)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for linear regression I need only number \n",
    "\n",
    "#do one-hot encoding for \"labor_force status\", but with catagegory being 0, Employed_missing, Unemployed and Other (=only being 6%)\n",
    "#do one hot-encodung for \"type_of_contratct\" with CDI, Unemployed, Employed_Missing, Other (=only being 12%)\n",
    "#do one_hot_encding for \"work_condition\" with Unemployed, P, Other(=only being 17%)\n",
    "#do one hot_econding for \"emp_contract\", with EMP1-6, Unemployed, EMP2-1, Other (=being 12%)\n",
    "#can keep as is for \"highest_credential\" because the ordinality make sense here for education\n",
    "#same for the location of insee code \n",
    "#use \"work condition\" instead of JOB_42 and then do one-hot-encdoding\n",
    "#do one-hot-encoding for \"household_type\"\n",
    "#for \"act\" do one hot encoding but maybe merge stay at home people with inactif + drop less than 14years olds as nobody in our data is \n",
    "#do one hot encoding for \"sport\" but with a \"Other\" category\n",
    "#do one hot encdoding for \"REG_JOB\" and \"FORMER_REG\"\n",
    "#do one hot encdoding for \"employer_category\" with 1 to 7 being \"Others\n",
    "#can keep it as is for \"employee count\" because here the ordinality make sense\n",
    "#do one hot encoding for \"Economic_sector_num\" and \"work_description\"\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# One-hot encoding for categorical variables, but why it is not dummy ????\n",
    "def preprocess_for_regression(data):\n",
    "    # One-hot encoding for \"labor_force_status\"\n",
    "    labor_force_status_ohe = pd.get_dummies(\n",
    "        data['labor_force_status'],\n",
    "        prefix='labor_force_status',\n",
    "        columns=['0', 'Employed_Missing', 'Unemployed', 'Other']\n",
    "    )\n",
    "    labor_force_status_ohe.astype(\"Int64\")\n",
    "\n",
    "    # One-hot encoding for \"type_of_contract\"\n",
    "    type_of_contract_ohe = pd.get_dummies(\n",
    "        data['TYPE_OF_CONTRACT'],\n",
    "        prefix='type_of_contract',\n",
    "        columns=['CDI', 'Unemployed', 'Employed_Missing', 'Other']\n",
    "    )\n",
    "\n",
    "    # One-hot encoding for \"work_condition\"\n",
    "    work_condition_ohe = pd.get_dummies(\n",
    "        data['WORK_CONDITION'],\n",
    "        prefix='work_condition',\n",
    "        columns=['Unemployed', 'P', 'Other']\n",
    "    )\n",
    "\n",
    "    # One-hot encoding for \"emp_contract\"\n",
    "    emp_contract_ohe = pd.get_dummies(\n",
    "        data['emp_contract'],\n",
    "        prefix='emp_contract',\n",
    "        columns=['EMP1-6', 'Unemployed', 'EMP2-1', 'Other']\n",
    "    )\n",
    "\n",
    "    # Keep \"highest_credential\" as is\n",
    "    # Keep \"INSEE_CODE\" as is\n",
    "    \n",
    "    # One-hot encoding for \"household_type\"\n",
    "    household_type_ohe = pd.get_dummies(\n",
    "        data['HOUSEHOLD_TYPE'],\n",
    "        prefix='HOUSEHOLD_TYPE'\n",
    "    )\n",
    "\n",
    "    # Merge \"stay at home\" with \"inactif\" in \"act\" and drop less than 14 years\n",
    "    data['act_num'] = data['act_num'].replace({'stay_at_home': 'inactif'})\n",
    "    data = data[data['age_2020'] >= 14]\n",
    "    act_ohe = pd.get_dummies(\n",
    "        data['act_num'],\n",
    "        prefix='act_num'\n",
    "    )\n",
    "\n",
    "    # One-hot encoding for \"sports\" with \"Other\" category\n",
    "    sports_ohe = pd.get_dummies(\n",
    "        data['Sports_Category'],\n",
    "        prefix='Sports_Category',\n",
    "        dummy_na=True\n",
    "    ).rename(columns={np.nan: 'Other'})\n",
    "\n",
    "    # One-hot encoding for \"REG_JOB\" and \"FORMER_REG\"\n",
    "    reg_job_ohe = pd.get_dummies(\n",
    "        data['REG_JOB'],\n",
    "        prefix='REG_JOB'\n",
    "    )\n",
    "    former_reg_ohe = pd.get_dummies(\n",
    "        data['REG_FORMER'],\n",
    "        prefix='FORMER_REG'\n",
    "    )\n",
    "\n",
    "    # One-hot encoding for \"employer_category\" with 1-7 as \"Others\"\n",
    "    employer_cat_type_ohe = pd.get_dummies(\n",
    "        data['Employer_category'],\n",
    "        prefix='Employer_category'\n",
    "    )\n",
    "    #data['Employer_category'] = data['Employer_category'].apply(lambda x: 'Others' if x in range(1, 8) else x)\n",
    "    #employer_category_ohe = pd.get_dummies(\n",
    "       #data['Employer_category'],\n",
    "        #prefix='Employer_category')\n",
    "\n",
    "    # Keep \"employee_count\" as is\n",
    "\n",
    "    # One-hot encoding for \"Economic_sector_num\"\n",
    "    economic_sector_num_ohe = pd.get_dummies(\n",
    "        data['Economic_sector_num'],\n",
    "        prefix='Economic_sector_num'\n",
    "    )\n",
    "\n",
    "    # One-hot encoding for \"work_description\"\n",
    "    work_description_ohe = pd.get_dummies(\n",
    "        data['work_description'],\n",
    "        prefix='work_description'\n",
    "    )\n",
    "\n",
    "    # Combine all encoded data with the original dataset\n",
    "    data = pd.concat(\n",
    "        [\n",
    "            data,\n",
    "            labor_force_status_ohe,\n",
    "            type_of_contract_ohe,\n",
    "            work_condition_ohe,\n",
    "            emp_contract_ohe,\n",
    "            household_type_ohe,\n",
    "            act_ohe,\n",
    "            sports_ohe,\n",
    "            reg_job_ohe,\n",
    "            former_reg_ohe,\n",
    "            employer_cat_type_ohe,\n",
    "            economic_sector_num_ohe,\n",
    "            work_description_ohe\n",
    "        ],\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    return data\n",
    "\n",
    "# Apply preprocessing to learn and test datasets\n",
    "learn_data_processed = preprocess_for_regression(learn_data)\n",
    "test_data_processed = preprocess_for_regression(test_data)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "def preprocess_for_regression(data):\n",
    "    # One-hot encoding for categorical variables\n",
    "    labor_force_status_ohe = pd.get_dummies(data['labor_force_status'], prefix='labor_force_status')\n",
    "    type_of_contract_ohe = pd.get_dummies(data['TYPE_OF_CONTRACT'], prefix='TYPE_OF_CONTRACT')\n",
    "    work_condition_ohe = pd.get_dummies(data['WORK_CONDITION'], prefix='WORK_CONDITION')\n",
    "    emp_contract_ohe = pd.get_dummies(data['emp_contract'], prefix='emp_contract')\n",
    "    household_type_ohe = pd.get_dummies(data['HOUSEHOLD_TYPE'], prefix='HOUSEHOLD_TYPE')\n",
    "    act_ohe = pd.get_dummies(data['act_num'], prefix='act_num')\n",
    "    sports_ohe = pd.get_dummies(data['Sports_Category'], prefix='Sports_Category', dummy_na=True).rename(\n",
    "        columns={np.nan: 'Sports_Category_Other'}\n",
    "    )\n",
    "    reg_job_ohe = pd.get_dummies(data['REG_JOB'], prefix='REG_JOB')\n",
    "    former_reg_ohe = pd.get_dummies(data['REG_FORMER'], prefix='FORMER_REG')\n",
    "    employer_cat_type_ohe = pd.get_dummies(data['Employer_category'], prefix='Employer_category')\n",
    "    economic_sector_num_ohe = pd.get_dummies(data['Economic_sector_num'], prefix='Economic_sector_num')\n",
    "    work_description_ohe = pd.get_dummies(data['work_description'], prefix='work_description')\n",
    "\n",
    "    # Combine all encoded data with the original dataset\n",
    "    encoded_data = pd.concat(\n",
    "        [\n",
    "            data,\n",
    "            labor_force_status_ohe,\n",
    "            type_of_contract_ohe,\n",
    "            work_condition_ohe,\n",
    "            emp_contract_ohe,\n",
    "            household_type_ohe,\n",
    "            act_ohe,\n",
    "            sports_ohe,\n",
    "            reg_job_ohe,\n",
    "            former_reg_ohe,\n",
    "            employer_cat_type_ohe,\n",
    "            economic_sector_num_ohe,\n",
    "            work_description_ohe,\n",
    "        ],\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # Drop original columns that were one-hot encoded\n",
    "    columns_to_drop = [\n",
    "        'labor_force_status', 'TYPE_OF_CONTRACT', 'WORK_CONDITION',\n",
    "        'emp_contract', 'HOUSEHOLD_TYPE', 'act_num', 'Sports_Category',\n",
    "        'REG_JOB', 'REG_FORMER', 'Employer_category', 'Economic_sector_num', 'work_description'\n",
    "    ]\n",
    "    encoded_data = encoded_data.drop(columns=columns_to_drop)\n",
    "\n",
    "    return encoded_data\n",
    "\n",
    "\n",
    "# Apply preprocessing to learn and test datasets\n",
    "learn_data_processed = preprocess_for_regression(learn_data)\n",
    "test_data_processed = preprocess_for_regression(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Transformer for Handling Imputation with Averages\n",
    "class ImputeMissingValues(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        # Calculate averages for training data (ignoring zero values)\n",
    "        self.avg_working_hours = X.loc[X[\"WORKING_HOURS\"] != 0].groupby(\"JOB_42\")[\"WORKING_HOURS\"].mean().round()\n",
    "        self.avg_pay = X.loc[X[\"Pay\"] != 0].groupby(\"JOB_42\")[\"Pay\"].mean().round()\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        # Use the stored averages to impute missing values\n",
    "        X = X.copy()\n",
    "        X[\"WORKING_HOURS\"] = X.apply(\n",
    "            lambda row: self.avg_working_hours.get(row[\"JOB_42\"], np.nan) \n",
    "            if pd.isnull(row[\"WORKING_HOURS\"]) else row[\"WORKING_HOURS\"],\n",
    "            axis=1\n",
    "        )\n",
    "        X[\"Pay\"] = X.apply(\n",
    "            lambda row: self.avg_pay.get(row[\"JOB_42\"], np.nan) \n",
    "            if pd.isnull(row[\"Pay\"]) else row[\"Pay\"],\n",
    "            axis=1\n",
    "        )\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Transformer for MICE (Multiple Imputation by Chained Equations) Imputation\n",
    "class MICEImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns_to_impute):\n",
    "        self.columns_to_impute = columns_to_impute\n",
    "        self.imputer = IterativeImputer(random_state=100)\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        # Fit the imputer only on the specified columns\n",
    "        self.imputer.fit(X[self.columns_to_impute])\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # Transform only the specified columns\n",
    "        X_imputed = X.copy()\n",
    "        X_imputed[self.columns_to_impute] = self.imputer.transform(X[self.columns_to_impute])\n",
    "        return X_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PRIMARY_KEY</th>\n",
       "      <th>sex</th>\n",
       "      <th>JOB_42</th>\n",
       "      <th>studying</th>\n",
       "      <th>age_2020</th>\n",
       "      <th>HOUSEHOLD_TYPE</th>\n",
       "      <th>target</th>\n",
       "      <th>act_num</th>\n",
       "      <th>HIGHEST_CREDENTIAL_num</th>\n",
       "      <th>RESIDENTS</th>\n",
       "      <th>...</th>\n",
       "      <th>Sports_Category</th>\n",
       "      <th>Employer_category</th>\n",
       "      <th>employee_count</th>\n",
       "      <th>TYPE_OF_CONTRACT</th>\n",
       "      <th>WORK_CONDITION</th>\n",
       "      <th>labor_force_status</th>\n",
       "      <th>WORKING_HOURS</th>\n",
       "      <th>REG_JOB</th>\n",
       "      <th>Economic_sector_num</th>\n",
       "      <th>work_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>csp_5_4</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>3</td>\n",
       "      <td>9.367020</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>14514</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CDI</td>\n",
       "      <td>C</td>\n",
       "      <td>O</td>\n",
       "      <td>1470.0</td>\n",
       "      <td>84</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>csp_6_3</td>\n",
       "      <td>0</td>\n",
       "      <td>80</td>\n",
       "      <td>7</td>\n",
       "      <td>8.648771</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>14514</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CDI</td>\n",
       "      <td>C</td>\n",
       "      <td>O</td>\n",
       "      <td>793.0</td>\n",
       "      <td>44</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>csp_3_1</td>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "      <td>4</td>\n",
       "      <td>10.792503</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>14514</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Employed_Missing</td>\n",
       "      <td>Employed_Missing</td>\n",
       "      <td>Employed_Missing</td>\n",
       "      <td>1241.0</td>\n",
       "      <td>Employed_Missing</td>\n",
       "      <td>Employed_Missing</td>\n",
       "      <td>Employed_Missing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>csp_3_7</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>4</td>\n",
       "      <td>8.508222</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>14514</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CDI</td>\n",
       "      <td>C</td>\n",
       "      <td>O</td>\n",
       "      <td>1201.0</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>csp_5_6</td>\n",
       "      <td>0</td>\n",
       "      <td>68</td>\n",
       "      <td>7</td>\n",
       "      <td>8.966272</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>14514</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Unemployed</td>\n",
       "      <td>Unemployed</td>\n",
       "      <td>Unemployed</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Unemployed</td>\n",
       "      <td>Unemployed</td>\n",
       "      <td>Unemployed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50038</th>\n",
       "      <td>100077</td>\n",
       "      <td>0</td>\n",
       "      <td>csp_8_5</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>5</td>\n",
       "      <td>11.154438</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>28540</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Unemployed</td>\n",
       "      <td>Unemployed</td>\n",
       "      <td>Unemployed</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Unemployed</td>\n",
       "      <td>Unemployed</td>\n",
       "      <td>Unemployed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50039</th>\n",
       "      <td>100078</td>\n",
       "      <td>0</td>\n",
       "      <td>csp_4_3</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>4</td>\n",
       "      <td>9.962345</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>28540</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>CDI</td>\n",
       "      <td>P</td>\n",
       "      <td>O</td>\n",
       "      <td>665.0</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50040</th>\n",
       "      <td>100079</td>\n",
       "      <td>0</td>\n",
       "      <td>csp_5_2</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>6</td>\n",
       "      <td>12.226288</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>28540</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>CDI</td>\n",
       "      <td>C</td>\n",
       "      <td>O</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50041</th>\n",
       "      <td>100081</td>\n",
       "      <td>1</td>\n",
       "      <td>csp_8_5</td>\n",
       "      <td>0</td>\n",
       "      <td>57</td>\n",
       "      <td>3</td>\n",
       "      <td>8.965529</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>28540</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Unemployed</td>\n",
       "      <td>Unemployed</td>\n",
       "      <td>Unemployed</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Unemployed</td>\n",
       "      <td>Unemployed</td>\n",
       "      <td>Unemployed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50042</th>\n",
       "      <td>100082</td>\n",
       "      <td>1</td>\n",
       "      <td>csp_6_2</td>\n",
       "      <td>0</td>\n",
       "      <td>51</td>\n",
       "      <td>1</td>\n",
       "      <td>11.928150</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>28540</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>CDI</td>\n",
       "      <td>C</td>\n",
       "      <td>O</td>\n",
       "      <td>1796.0</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50043 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       PRIMARY_KEY  sex   JOB_42  studying  age_2020  HOUSEHOLD_TYPE  \\\n",
       "0                1    0  csp_5_4         0        34               3   \n",
       "1                5    1  csp_6_3         0        80               7   \n",
       "2                7    0  csp_3_1         0        63               4   \n",
       "3                8    1  csp_3_7         0        50               4   \n",
       "4               10    0  csp_5_6         0        68               7   \n",
       "...            ...  ...      ...       ...       ...             ...   \n",
       "50038       100077    0  csp_8_5         1        29               5   \n",
       "50039       100078    0  csp_4_3         0        34               4   \n",
       "50040       100079    0  csp_5_2         0        29               6   \n",
       "50041       100081    1  csp_8_5         0        57               3   \n",
       "50042       100082    1  csp_6_2         0        51               1   \n",
       "\n",
       "          target  act_num  HIGHEST_CREDENTIAL_num  RESIDENTS  ...  \\\n",
       "0       9.367020        1                      10      14514  ...   \n",
       "1       8.648771        3                       6      14514  ...   \n",
       "2      10.792503        1                      11      14514  ...   \n",
       "3       8.508222        1                       9      14514  ...   \n",
       "4       8.966272        2                       6      14514  ...   \n",
       "...          ...      ...                     ...        ...  ...   \n",
       "50038  11.154438        6                       6      28540  ...   \n",
       "50039   9.962345        1                       9      28540  ...   \n",
       "50040  12.226288        1                       8      28540  ...   \n",
       "50041   8.965529        7                       6      28540  ...   \n",
       "50042  11.928150        1                       6      28540  ...   \n",
       "\n",
       "       Sports_Category  Employer_category employee_count  TYPE_OF_CONTRACT  \\\n",
       "0                    1                9.0            1.0               CDI   \n",
       "1                    0                9.0            1.0               CDI   \n",
       "2                    0                0.0            0.0  Employed_Missing   \n",
       "3                    0                9.0            1.0               CDI   \n",
       "4                    0               10.0            7.0        Unemployed   \n",
       "...                ...                ...            ...               ...   \n",
       "50038                0               10.0            7.0        Unemployed   \n",
       "50039                0                8.0            6.0               CDI   \n",
       "50040                0                8.0            4.0               CDI   \n",
       "50041                0               10.0            7.0        Unemployed   \n",
       "50042                0                9.0            3.0               CDI   \n",
       "\n",
       "         WORK_CONDITION labor_force_status  WORKING_HOURS           REG_JOB  \\\n",
       "0                     C                  O         1470.0                84   \n",
       "1                     C                  O          793.0                44   \n",
       "2      Employed_Missing   Employed_Missing         1241.0  Employed_Missing   \n",
       "3                     C                  O         1201.0                11   \n",
       "4            Unemployed         Unemployed            0.0        Unemployed   \n",
       "...                 ...                ...            ...               ...   \n",
       "50038        Unemployed         Unemployed            0.0        Unemployed   \n",
       "50039                 P                  O          665.0                11   \n",
       "50040                 C                  O         3000.0                11   \n",
       "50041        Unemployed         Unemployed            0.0        Unemployed   \n",
       "50042                 C                  O         1796.0                11   \n",
       "\n",
       "       Economic_sector_num  work_description  \n",
       "0                        7                 5  \n",
       "1                        4                 6  \n",
       "2         Employed_Missing  Employed_Missing  \n",
       "3                        2                 3  \n",
       "4               Unemployed        Unemployed  \n",
       "...                    ...               ...  \n",
       "50038           Unemployed        Unemployed  \n",
       "50039                    9                 4  \n",
       "50040                    9                 5  \n",
       "50041           Unemployed        Unemployed  \n",
       "50042                    2                 6  \n",
       "\n",
       "[50043 rows x 26 columns]"
      ]
     },
     "execution_count": 418,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specify the columns that need MICE imputation\n",
    "columns_to_impute = [\"Employer_category\", \"employee_count\"]\n",
    "\n",
    "# Define the combined pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('impute_averages', ImputeMissingValues()),  # Step 1: Impute using averages\n",
    "    ('mice_imputer', MICEImputer(columns_to_impute=columns_to_impute)),  # Step 2: MICE imputation\n",
    "])\n",
    "\n",
    "pipeline.fit_transform(learn_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this gives 0 and 1\n",
    "\n",
    "def preprocess_for_regression(data):\n",
    "    # Initialize OneHotEncoder with sparse output disabled to return dense arrays\n",
    "    ohe = OneHotEncoder(sparse_output=False, dtype=int, handle_unknown='ignore')\n",
    "    \n",
    "    # List of columns to encode\n",
    "    categorical_columns = [\n",
    "        'HOUSEHOLD_TYPE', 'act_num', 'Sports_Category', \n",
    "        'Employer_category', \"employee_count\"\n",
    "    ]\n",
    "\n",
    "    # List of columns to encode categorical_columns = [\n",
    "        #'labor_force_status', 'TYPE_OF_CONTRACT', 'WORK_CONDITION', \n",
    "        #'emp_contract', 'HOUSEHOLD_TYPE', 'act_num', 'Sports_Category', \n",
    "        #'REG_JOB', 'REG_FORMER', 'Employer_category', 'Economic_sector_num', \n",
    "        #'work_description' ]\n",
    "    \n",
    "    # Apply the encoder\n",
    "    encoded_data = ohe.fit_transform(data[categorical_columns])\n",
    "    \n",
    "    # Create a DataFrame for the encoded features\n",
    "    encoded_feature_names = ohe.get_feature_names_out(categorical_columns)\n",
    "    encoded_df = pd.DataFrame(encoded_data, columns=encoded_feature_names, index=data.index)\n",
    "    \n",
    "    # Drop original categorical columns from the dataset\n",
    "    data = data.drop(columns=categorical_columns)\n",
    "    \n",
    "    # Merge encoded features with the original dataset\n",
    "    processed_data = pd.concat([data, encoded_df], axis=1)\n",
    "    \n",
    "    return processed_data\n",
    "\n",
    "\n",
    "# Apply preprocessing to learn and test datasets\n",
    "learn_data = preprocess_for_regression(learn_data)\n",
    "#test_data = preprocess_for_regression(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we might need to rescale working_hours, pay and employee_count\n",
    "#risk multicollinearity so check for highly correlated features and consider dropping or combining them.\n",
    "#might need to use PCA to reduce dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = learn_data.drop(columns=[\"target\"])\n",
    "y_train = learn_data[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(x_train,\n",
    "                                                    y_train, \n",
    "                                                    test_size=0.3, \n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_folds = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize the RandomForestRegressor model\n",
    "model = LinearRegression()  #need to add that in the pipeline \n",
    "\n",
    "# Set up the GridSearchCV for RandomForestRegressor with appropriate scoring metric\n",
    "#rf_search = GridSearchCV(model, cv=cv_folds, n_jobs=-1, scoring=\"mean_squared_error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "boolean value of NA is ambiguous",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[424], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m rf_res \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(X_train, Y_train)\n",
      "File \u001b[1;32mc:\\Users\\graci\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\graci\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_base.py:578\u001b[0m, in \u001b[0;36mLinearRegression.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    574\u001b[0m n_jobs_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs\n\u001b[0;32m    576\u001b[0m accept_sparse \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositive \u001b[38;5;28;01melse\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoo\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m--> 578\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[0;32m    579\u001b[0m     X, y, accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse, y_numeric\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, multi_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    580\u001b[0m )\n\u001b[0;32m    582\u001b[0m has_sw \u001b[38;5;241m=\u001b[39m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_sw:\n",
      "File \u001b[1;32mc:\\Users\\graci\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:650\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    648\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[0;32m    649\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 650\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m check_X_y(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    651\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[1;32mc:\\Users\\graci\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1263\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1258\u001b[0m         estimator_name \u001b[38;5;241m=\u001b[39m _check_estimator_name(estimator)\n\u001b[0;32m   1259\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1260\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1261\u001b[0m     )\n\u001b[1;32m-> 1263\u001b[0m X \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[0;32m   1264\u001b[0m     X,\n\u001b[0;32m   1265\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[0;32m   1266\u001b[0m     accept_large_sparse\u001b[38;5;241m=\u001b[39maccept_large_sparse,\n\u001b[0;32m   1267\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m   1268\u001b[0m     order\u001b[38;5;241m=\u001b[39morder,\n\u001b[0;32m   1269\u001b[0m     copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[0;32m   1270\u001b[0m     force_all_finite\u001b[38;5;241m=\u001b[39mforce_all_finite,\n\u001b[0;32m   1271\u001b[0m     ensure_2d\u001b[38;5;241m=\u001b[39mensure_2d,\n\u001b[0;32m   1272\u001b[0m     allow_nd\u001b[38;5;241m=\u001b[39mallow_nd,\n\u001b[0;32m   1273\u001b[0m     ensure_min_samples\u001b[38;5;241m=\u001b[39mensure_min_samples,\n\u001b[0;32m   1274\u001b[0m     ensure_min_features\u001b[38;5;241m=\u001b[39mensure_min_features,\n\u001b[0;32m   1275\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mestimator,\n\u001b[0;32m   1276\u001b[0m     input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1277\u001b[0m )\n\u001b[0;32m   1279\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[0;32m   1281\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[1;32mc:\\Users\\graci\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1049\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1043\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1044\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1045\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[0;32m   1046\u001b[0m     )\n\u001b[0;32m   1048\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[1;32m-> 1049\u001b[0m     _assert_all_finite(\n\u001b[0;32m   1050\u001b[0m         array,\n\u001b[0;32m   1051\u001b[0m         input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[0;32m   1052\u001b[0m         estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[0;32m   1053\u001b[0m         allow_nan\u001b[38;5;241m=\u001b[39mforce_all_finite \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1054\u001b[0m     )\n\u001b[0;32m   1056\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[0;32m   1057\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[0;32m   1058\u001b[0m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\graci\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:110\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;66;03m# for object dtype data, we only check for NaNs (GH-13254)\u001b[39;00m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_nan:\n\u001b[1;32m--> 110\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _object_dtype_isnan(X)\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m    111\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput contains NaN\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;66;03m# We need only consider float arrays, hence can early return for all else.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\graci\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\fixes.py:60\u001b[0m, in \u001b[0;36m_object_dtype_isnan\u001b[1;34m(X)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_object_dtype_isnan\u001b[39m(X):\n\u001b[1;32m---> 60\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m X \u001b[38;5;241m!=\u001b[39m X\n",
      "File \u001b[1;32mmissing.pyx:392\u001b[0m, in \u001b[0;36mpandas._libs.missing.NAType.__bool__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: boolean value of NA is ambiguous"
     ]
    }
   ],
   "source": [
    "rf_res = model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "# Get the best model after GridSearchCV\n",
    "#best_model_rf = rf_res.best_estimator_\n",
    "\n",
    "# Output the best model and its score\n",
    "print(f\"Best model: {rf_res}\")\n",
    "print(f\"Best score: {rf_res.best_score_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions = rf_res.predict(X_train)\n",
    "print(\"R² for training set: \", r2_score(Y_train, train_predictions))\n",
    "print(\"RMSE on the learning set:\", root_mean_squared_error(Y_train, rf_res.predict(X_train)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
