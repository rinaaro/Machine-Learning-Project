{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the directory containing the CSV files\n",
    "files_folder = \"files\"\n",
    "\n",
    "# Iterate over all files in the folder\n",
    "for file_name in os.listdir(files_folder):\n",
    "    if file_name.endswith('.csv'):  # Check if the file is a CSV\n",
    "        file_path = os.path.join(files_folder, file_name)\n",
    "        # Create a variable with the name of the CSV file (minus extension)\n",
    "        df_name = os.path.splitext(file_name)[0]\n",
    "        globals()[df_name] = pd.read_csv(file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CHECKING DATA TYPES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes_list = [var_name for var_name in globals() if isinstance(globals()[var_name], pd.DataFrame)]\n",
    "\n",
    "for df_name in dataframes_list:\n",
    "    df = globals()[df_name]\n",
    "    print(f\"DataFrame name: {df_name}\")\n",
    "    print(df.dtypes)\n",
    "    print(\"-\" * 40) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CHECKING MISSING VALUES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "METHOD 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_values_table(df):\n",
    "        mis_val = df.isnull().sum()\n",
    "        mis_val_percent = 100 * df.isnull().sum() / len(df)  #% of missing values\n",
    "        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)  #create result table\n",
    "        mis_val_table_ren_columns = mis_val_table.rename(\n",
    "        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n",
    "        mis_val_table_ren_columns = mis_val_table_ren_columns[\n",
    "            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n",
    "        '% of Total Values', ascending=False).round(1)\n",
    "        print (\"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n",
    "              \" columns that have missing values.\")\n",
    "        return mis_val_table_ren_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = [var_name for var_name in globals() if isinstance(globals()[var_name], pd.DataFrame)]\n",
    "\n",
    "for df_name in dataframe:\n",
    "    df = globals()[df_name]\n",
    "    mis_val = df.isnull().sum()\n",
    "    if mis_val.sum() > 0: \n",
    "        print(f\"Missing values in {df_name}:\")\n",
    "        print(missing_values_table(df))  # Pass the actual DataFrame to the function\n",
    "        print(\"-\" * 40)  #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "METHOD 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = [var_name for var_name in globals() if isinstance(globals()[var_name], pd.DataFrame)]\n",
    "\n",
    "for df_name in dataframe:\n",
    "    df = globals()[df_name]  \n",
    "    missing_counts = df.isnull().sum()\n",
    "    if missing_counts.sum() > 0:                               #Check for NA values\n",
    "        print(f\"Missing values in {df_name}:\")\n",
    "        print(missing_counts[missing_counts > 0])                #Print only df with NA values\n",
    "        print(\"-\" * 40) \n",
    "#NA values in : learn_dataset_job, learn_dataset_retired_jobs, test_dataset_job, test_dataset_retired_jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HANDLING MISSING VALUES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CATEGORICAL VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(learn_dataset_retired_jobs.loc[:,\"employee_count\"])\n",
    "dict(zip(le.classes_, le.transform(le.classes_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_dataset_retired_jobs['employee_count_encoded'] = le.transform(learn_dataset_retired_jobs['employee_count'])\n",
    "learn_dataset_retired_jobs['employee_count_encoded'] = learn_dataset_retired_jobs['employee_count_encoded'].map(lambda x: np.nan if x==7 else x)\n",
    "df_train = learn_dataset_retired_jobs.loc[:,[\"PRIMARY_KEY\", \"employee_count_encoded\",\"WORKING_HOURS\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The issue isthat KNN only works for numerical columns but no value to do K-NN with just those 3 variables, ig we need to encode the rest of colums\n",
    "df_train = learn_dataset_retired_jobs.loc[:,[\"PRIMARY_KEY\", \"employee_count_encoded\",\"WORKING_HOURS\"]]\n",
    "knn_imputer = KNNImputer(n_neighbors=5, weights=\"uniform\")\n",
    "knn_imputer.fit(df_train)\n",
    "#learn_dataset_retired_jobs[f'{column}_encoded'] = knn_imputer.fit_transform(learn_dataset_retired_jobs[[f'{column}_encoded']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_imputer = KNNImputer(n_neighbors=5, weights=\"uniform\")\n",
    "knn_imputer.fit(df_train)\n",
    "#learn_dataset_retired_jobs[f'{column}_encoded'] = knn_imputer.fit_transform(learn_dataset_retired_jobs[[f'{column}_encoded']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_imputer = KNNImputer(n_neighbors=5, weights=\"uniform\")\n",
    "knn_imputer.fit(df_train)\n",
    "#learn_dataset_retired_jobs[f'{column}_encoded'] = knn_imputer.fit_transform(learn_dataset_retired_jobs[[f'{column}_encoded']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_imputed = list(le.inverse_transform(learn_dataset_retired_jobs['employee_count_encoded'].round().astype('int')))\n",
    "learn_dataset_retired_jobs[\"employee_count_encoded\"] = count_imputed\n",
    "learn_dataset_retired_jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NUMERICAL VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputation for rows with only 1 NA in 'WORKING_HOURS'\n",
    "df_w_na = ['learn_dataset_job', 'learn_dataset_retired_jobs', 'test_dataset_job', 'test_dataset_retired_jobs']\n",
    "\n",
    "for df_w_na_name in df_w_na:\n",
    "    df = globals()[df_w_na_name]\n",
    "    if 'WORKING_HOURS' in df.columns:\n",
    "        rows_with_one_na_in_working_hours = df[df['WORKING_HOURS'].isnull() & df.isnull().sum(axis=1) == 1]\n",
    "        if not rows_with_one_na_in_working_hours.empty: \n",
    "            mean_imputer = SimpleImputer(strategy='mean')\n",
    "            # Perform the imputation and assign the result to the corresponding rows and column\n",
    "            imputed_values = mean_imputer.fit_transform(rows_with_one_na_in_working_hours[['WORKING_HOURS']])\n",
    "            # Assign the imputed values back to the DataFrame\n",
    "            df.loc[rows_with_one_na_in_working_hours.index, 'WORKING_HOURS'] = imputed_values.flatten()  # Flatten the 2D array\n",
    "            print(f\"Imputed missing values for 'WORKING_HOURS' in {df_w_na_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imputation with mean for rows with only 1 NA values (the NA values being for 'WORKING_HOURS')\n",
    "df_w_na = ['learn_dataset_job', 'learn_dataset_retired_jobs', 'test_dataset_job', 'test_dataset_retired_jobs']\n",
    "\n",
    "for df_w_na_name in df_w_na:\n",
    "    df = globals()[df_w_na_name]\n",
    "    if 'WORKING_HOURS' in df.columns:\n",
    "        rows_with_one_na_in_working_hours = df[df['WORKING_HOURS'].isnull() & df.isnull().sum(axis=1) == 1]\n",
    "        if not rows_with_one_na_in_working_hours.empty: \n",
    "            mean_imputer = SimpleImputer(strategy='mean')\n",
    "            df.loc[rows_with_one_na_in_working_hours.index, 'WORKING_HOURS'] = mean_imputer.fit_transform(\n",
    "                rows_with_one_na_in_working_hours[['WORKING_HOURS']])\n",
    "    print(f\"Imputed missing values for 'WORKING_HOURS' in {df_w_na_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
