{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install fancyimpute scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split, ShuffleSplit, KFold, cross_val_score, GridSearchCV, StratifiedKFold\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import root_mean_squared_error, r2_score, mean_squared_error\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.impute import SimpleImputer, IterativeImputer\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#impute for 400+ ppl missing FORMER_REG + JOB_REG - done \n",
    "#do imputation for people with \"Retired_missing\" + \"Employed_missing\" labels\n",
    "#check the command about encoding part linear model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_name in os.listdir(\"files\"):\n",
    "    if file_name.endswith('.csv'):\n",
    "        file_path = os.path.join(\"files\", file_name)\n",
    "\n",
    "        df_name = os.path.splitext(file_name)[0]\n",
    "        globals()[df_name] = pd.read_csv(file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA CLEANING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simplification of categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge sports data and extract sports categories\n",
    "def extract_sports_category(dataset, sports_code):\n",
    "    merged = pd.merge(dataset, sports_code, left_on=\"Sports\", right_on=\"Code\")\n",
    "    merged[\"Sports_Category\"] = merged[\"Categorie\"]\n",
    "    return merged[[\"PRIMARY_KEY\", \"Sports_Category\"]]\n",
    "\n",
    "learn_sports = extract_sports_category(learn_dataset_sport, code_Sports)\n",
    "test_sports = extract_sports_category(test_dataset_sport, code_Sports)\n",
    "\n",
    "# Merge departments into regions and extract relevant region columns\n",
    "def merge_and_extract_region(df, merge_column, region_column_name):\n",
    "    merged = pd.merge(df, departments, left_on=merge_column, right_on=\"DEP\", how=\"left\")\n",
    "    merged[region_column_name] = merged[\"REG\"]\n",
    "    return merged.drop([\"Nom du département\", \"REG\", \"DEP\", merge_column], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_dataset_job = merge_and_extract_region(learn_dataset_job, \"JOB_DEP\", \"JOB_REG\")    #put the column as int \n",
    "learn_dataset_retired_jobs = merge_and_extract_region(learn_dataset_retired_jobs, \"JOB_DEP\", \"JOB_REG\")\n",
    "learn_dataset_retired_jobs = merge_and_extract_region(learn_dataset_retired_jobs, \"FORMER_DEP\", \"FORMER_REG\")\n",
    "\n",
    "test_dataset_job = merge_and_extract_region(test_dataset_job, \"JOB_DEP\", \"JOB_REG\")\n",
    "test_dataset_retired_jobs = merge_and_extract_region(test_dataset_retired_jobs, \"JOB_DEP\", \"JOB_REG\")\n",
    "test_dataset_retired_jobs = merge_and_extract_region(test_dataset_retired_jobs, \"FORMER_DEP\", \"FORMER_REG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Economic sector into fewer categories (and numeric instead of object/string)\n",
    "def sector_mapping(nace_code):\n",
    "    if nace_code == \"AZ\":  \n",
    "        return \"Agriculture, forestry and fishing)\"\n",
    "    elif \"BZ\" <= nace_code <= \"EZ\":\n",
    "        return \"Manufacturing, mining and quarrying and other industrial activities\"\n",
    "    elif nace_code == \"FZ\": \n",
    "        return \"Construction\"\n",
    "    elif \"GZ\" <= nace_code <= \"IZ\":  \n",
    "        return \"Wholesale and retail trade, transportation and storage, accommodation and food service activities\"\n",
    "    elif \"JA\" <= nace_code <= \"JC\":\n",
    "        return \"Information and communication\"\n",
    "    elif nace_code == \"KZ\": \n",
    "        return \"Financial and insurance activities\"\n",
    "    elif nace_code == \"LZ\": \n",
    "        return \"Real estate activities\"\n",
    "    elif \"MA\" <= nace_code <= \"NZ\":\n",
    "        return \"Professional, scientific, technical, administrative and support service activities\"\n",
    "    elif \"OZ\" <= nace_code <= \"QB\":\n",
    "        return \"Public administration and defence, education, human health and social work activities\"\n",
    "    elif \"RZ\" <= nace_code <= \"UZ\":\n",
    "        return \"Other services activities\"\n",
    "    else:\n",
    "        return \"Unknown Sector\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_Economic_sector[\"Nomenclature\"] = code_Economic_sector[\"Code\"].map(sector_mapping)\n",
    "code_Economic_sector[\"Economic_sector_num\"] = pd.factorize(code_Economic_sector[\"Nomenclature\"])[0] + 1\n",
    "\n",
    "#issue in the code IZ should be its own category \"Hébergement et restauration\"\n",
    "#HZ, JZ à MC so LZ, KZ, MA, MB and MC, NZ, OZ, PZ,  QA, QB, RZ, SZ, TZ et UZ should be in the same category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_HIGHEST_CREDENTIAL[\"HIGHEST_CREDENTIAL_num\"] = pd.factorize(code_HIGHEST_CREDENTIAL[\"Code\"])[0] + 1\n",
    "code_act[\"act_num\"] = pd.factorize(code_act[\"Code\"])[0] + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_data = learn_dataset\n",
    "\n",
    "learn_data = pd.merge(learn_data, code_act, left_on=\"act\", right_on=\"Code\", how=\"left\")\n",
    "learn_data.drop([\"Code\", \"Libellé\"], axis=1, inplace=True)  #should also drop act ?\n",
    "learn_data = pd.merge(learn_data, code_HIGHEST_CREDENTIAL, left_on=\"HIGHEST_CREDENTIAL\", right_on=\"Code\", how=\"left\")\n",
    "learn_data.drop([\"Code\", \"Libellé\", \"HIGHEST_CREDENTIAL\"], axis=1, inplace=True)\n",
    "\n",
    "#for imputation fitting\n",
    "learn_data = pd.merge(learn_data, city_pop, on=\"INSEE_CODE\", how=\"left\")\n",
    "learn_data = pd.merge(learn_data, city_loc, on=\"INSEE_CODE\", how=\"left\")\n",
    "learn_data = pd.merge(learn_data, city_adm, on=\"INSEE_CODE\", how=\"left\")\n",
    "learn_data = merge_and_extract_region(learn_data, \"DEP\", \"CURRENT_REG\")\n",
    "\n",
    "test_data = test_dataset\n",
    "\n",
    "test_data = pd.merge(test_data, code_act, left_on=\"act\", right_on=\"Code\", how=\"left\")\n",
    "test_data.drop([\"Code\", \"Libellé\"], axis=1, inplace=True)\n",
    "test_data = pd.merge(test_data, code_HIGHEST_CREDENTIAL, left_on=\"HIGHEST_CREDENTIAL\", right_on=\"Code\", how=\"left\")\n",
    "test_data.drop([\"Code\", \"Libellé\", \"HIGHEST_CREDENTIAL\"], axis=1, inplace=True)\n",
    "\n",
    "test_data = pd.merge(test_data, city_pop, on=\"INSEE_CODE\", how=\"left\")\n",
    "test_data = pd.merge(test_data, city_loc, on=\"INSEE_CODE\", how=\"left\")\n",
    "test_data = pd.merge(test_data, city_adm, on=\"INSEE_CODE\", how=\"left\")\n",
    "test_data = merge_and_extract_region(test_data, \"DEP\", \"CURRENT_REG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_dfs = [learn_dataset_emp_contract, learn_dataset_job, learn_dataset_retired_former, \n",
    "             learn_dataset_retired_jobs, learn_dataset_retired_pension, learn_sports]\n",
    "\n",
    "test_dfs = [test_dataset_emp_contract, test_dataset_job, test_dataset_retired_former, \n",
    "            test_dataset_retired_jobs, test_dataset_retired_pension, test_sports]\n",
    "\n",
    "for df in learn_dfs:\n",
    "    learn_data = pd.merge(learn_data, df, on=\"PRIMARY_KEY\", how=\"outer\")\n",
    "\n",
    "for df in test_dfs:\n",
    "    test_data = pd.merge(test_data, df, on=\"PRIMARY_KEY\", how=\"outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to combine columns ending with `_x` and `_y` into a single base column\n",
    "def combine_duplicate_columns(dataframe):\n",
    "    for column in dataframe.columns:\n",
    "        if column.endswith('_x'):\n",
    "            base_column = column[:-2]  # Remove `_x` suffix\n",
    "            y_column = base_column + '_y'\n",
    "            if y_column in dataframe.columns:\n",
    "                # Combine the `_x` and `_y` columns\n",
    "                dataframe[base_column] = dataframe[column].fillna(dataframe[y_column])\n",
    "                # Drop the original `_x` and `_y` columns\n",
    "                dataframe.drop(columns=[column, y_column], inplace=True)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_data = combine_duplicate_columns(learn_data)\n",
    "test_data = combine_duplicate_columns(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formatting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def household_num(value):\n",
    "    parts = value.split('|')  # Split the value by '|'\n",
    "    if parts[1] in {'1', '2', '3'}:  # For M|1|-- to M|3|--\n",
    "        return int(parts[1])\n",
    "    elif parts[1] == '4':  # For M|4|1 to M|4|4\n",
    "        return 4 + (int(parts[2]) - 1)  # 4 + (1-1), 4 + (2-1), etc.\n",
    "    return None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_HOUSEHOLD_TYPE['HOUSEHOLD_TYPE_num'] = code_HOUSEHOLD_TYPE['Code'].apply(household_num)\n",
    "learn_data['HOUSEHOLD_TYPE'] = learn_data['HOUSEHOLD_TYPE'].apply(household_num)\n",
    "test_data['HOUSEHOLD_TYPE'] = test_data['HOUSEHOLD_TYPE'].apply(household_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_columns(primary_col, fallback_col): #this is imputation\n",
    "    \"\"\"Combine two columns, filling missing values in the primary column with values from the fallback column.\"\"\"\n",
    "    return primary_col.fillna(fallback_col) if fallback_col is not None else primary_col \n",
    "\n",
    "def preprocess_employee_data(data, economic_sector_code, work_description_map):\n",
    "    data[\"employee_count\"] = data[\"employee_count\"].str.extract(r'tr_(\\d)')[0].astype(\"Int64\")\n",
    "    data[\"Employer_category\"] = data[\"Employer_category\"].str.extract(r'ct_(\\d)')[0].astype(\"Int64\")\n",
    "    \n",
    "    \n",
    "    data = data.merge(economic_sector_code, left_on=\"Economic_sector\", right_on=\"Code\", how=\"left\")\n",
    "    \n",
    "    # Merge with work description map and clean up columns\n",
    "    data = data.merge(work_description_map, left_on=\"work_description\", right_on=\"N3\", how=\"left\")\n",
    "    data.drop([\"work_description\", \"N3\", \"N2\"], axis=1, inplace=True)\n",
    "    data[\"work_description\"] = data[\"N1\"].str.extract(r'csp_(\\d)')[0].astype(\"Int64\")\n",
    "    data.drop(\"N1\", axis=1, inplace=True)\n",
    "    \n",
    "    # Combine relevant columns for contracts and pay\n",
    "    data[\"emp_contract\"] = combine_columns(data[\"emp_contract\"], data[\"former_emp_contract\"])\n",
    "    data[\"Pay\"] = combine_columns(data[\"Pay\"], data[\"RETIREMENT_PAY\"])\n",
    "    data[\"JOB_REG\"] = combine_columns(data[\"JOB_REG\"], data[\"FORMER_REG\"])  \n",
    "    data[\"FORMER_REG\"] = combine_columns(data[\"FORMER_REG\"], data[\"JOB_REG\"])\n",
    "    data.loc[~data['JOB_42'].astype(str).str.startswith('csp_7'), 'JOB_REG'] = combine_columns(data['JOB_REG'], data['CURRENT_REG'])\n",
    " \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_data = preprocess_employee_data(learn_data, code_Economic_sector, code_work_description_map)\n",
    "test_data = preprocess_employee_data(test_data, code_Economic_sector, code_work_description_map)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "#JUST CHECK THAT WE DID NOT PU 0 TO RETIREES THAT WE SHOULD HAVE IMPUTED \n",
    "def categorize_retirement_age(data):\n",
    "    data['retirement_age'] = pd.to_numeric(data['retirement_age'], errors='coerce')\n",
    "    \n",
    "    bins = [0, 57, 60, 61, 63, 65, float('inf')]  # Specify edges for the ranges\n",
    "    labels = ['<57', '57-59', '60', '61-62', '63-64', '65+']  # Labels for ranges\n",
    "\n",
    "    # Categorize retirement_age into retirement_age_cat\n",
    "    data['retirement_age_cat'] = pd.cut(\n",
    "        data['retirement_age'], \n",
    "        bins=bins, \n",
    "        labels=labels, \n",
    "        right=False,  # Left-closed intervals\n",
    "        include_lowest=True\n",
    "    )\n",
    "    \n",
    "    # Ensure missing values in retirement_age_cat are handled properly\n",
    "    data['retirement_age_cat'] = data['retirement_age_cat'].astype(object)  # Avoid ambiguity with NA\n",
    "    \n",
    "    # Handle exact matches for 60 and 65\n",
    "    data.loc[data['retirement_age'] == 60, 'retirement_age_cat'] = '60'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "categorize_retirement_age(learn_data)\n",
    "categorize_retirement_age(test_data)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_column_types(data):\n",
    "    data[\"sex\"] = pd.factorize(data[\"sex\"])[0]\n",
    "    data[\"studying\"] = data[\"studying\"].astype(\"int64\")\n",
    "    data[\"Sports_Category\"] = data[\"Sports_Category\"].fillna(0).astype(\"int64\")\n",
    "    \n",
    "    # List of columns to convert to Int64\n",
    "    int_columns = [\"CURRENT_REG\", \"JOB_REG\", \"FORMER_REG\", \"retirement_age\", \"WORKING_HOURS\", \"Economic_sector_num\", \"Pay\"]\n",
    "    \n",
    "    for col in int_columns:\n",
    "        data[col] = pd.to_numeric(data[col], errors='coerce').astype('Int64')\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_data = preprocess_column_types(learn_data)\n",
    "test_data = preprocess_column_types(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labelling non-imputable values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_na_with_unemployed(data, column_name):\n",
    "    data[column_name] = data[column_name].astype('category')\n",
    "    \n",
    "    additional_categories = ['Unemployed']     \n",
    "    \n",
    "    # Add the specified categories\n",
    "    data[column_name] = data[column_name].cat.add_categories(additional_categories)\n",
    "    \n",
    "    # Assign categories based on conditions\n",
    "    data.loc[(data[column_name].isna()) & ((data['JOB_42'].astype(str).str.startswith('csp_8')) | (data['act_num'] == 2)), column_name] = 'Unemployed'\n",
    "\n",
    "# List of columns to process\n",
    "columns_to_process = [\n",
    "    \"emp_contract\", \"TYPE_OF_CONTRACT\", \"WORK_CONDITION\", \"Employer_category\",\n",
    "    \"labor_force_status\", \"Economic_sector_num\", \"JOB_REG\", \"employee_count\",\n",
    "    \"FORMER_REG\", \"work_description\", \"retirement_age\", \"FORMER_JOB_42\"\n",
    "]\n",
    "\n",
    "for column in columns_to_process:\n",
    "    replace_na_with_unemployed(learn_data, column)\n",
    "    replace_na_with_unemployed(test_data, column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_na_with_employed(data, column_name):\n",
    "    data[column_name] = data[column_name].astype('category')\n",
    "    additional_categories = ['Employed']     \n",
    "    data[column_name] = data[column_name].cat.add_categories(additional_categories)\n",
    "    data.loc[(data[column_name].isna()) & (data['act_num'] == 1), column_name] = 'Employed'\n",
    "\n",
    "columns_to_process = [ \n",
    "    \"FORMER_REG\", \"retirement_age\", \"FORMER_JOB_42\"\n",
    "]\n",
    "\n",
    "for column in columns_to_process:\n",
    "    replace_na_with_employed(learn_data, column)\n",
    "    replace_na_with_employed(test_data, column)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def replace_na_with_employed(data, column_name):\n",
    "    # Convert the column to categorical\n",
    "    data[column_name] = data[column_name].astype('category')\n",
    "    \n",
    "    # Define categories to add\n",
    "    additional_categories = ['Retired']     \n",
    "    \n",
    "    # Add the specified categories\n",
    "    data[column_name] = data[column_name].cat.add_categories(additional_categories)\n",
    "    \n",
    "    # Assign categories based on conditions\n",
    "    \n",
    "    data.loc[(data[column_name].isna()) & (data['JOB_42'].astype(str).str.startswith('csp_7'))] = 'Retired'\n",
    "\n",
    "# List of columns to process\n",
    "columns_to_process = [\n",
    "    \"emp_contract\", \"TYPE_OF_CONTRACT\", \"WORK_CONDITION\", \"N3\",\n",
    "    \"labor_force_status\", \"Economic_sector_num\", \"JOB_REG\", \n",
    "    \"FORMER_REG\", \"work_description\", \"retirement_age_cat\", \"FORMER_JOB_42\"\n",
    "]\n",
    "\n",
    "# Apply the function to each column for both datasets\n",
    "for column in columns_to_process:\n",
    "    replace_na_with_employed(learn_data, column)\n",
    "    replace_na_with_employed(test_data, column)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def fill_missing_values(data, column, unemployed_value, default_value):\n",
    "    data[column] = data.apply(\n",
    "        lambda row: unemployed_value if pd.isna(row[column]) and \n",
    "                        (str(row['JOB_42']).startswith('csp_8') or row['act_num'] == 2)  # unemployed\n",
    "                    else (default_value if pd.isna(row[column]) else row[column]),\n",
    "        axis=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fill_missing_values(learn_data, 'Employer_category', unemployed_value=10, default_value=0)\n",
    "fill_missing_values(learn_data, 'employee_count', unemployed_value=7, default_value=0)\n",
    "\n",
    "fill_missing_values(test_data, 'Employer_category', unemployed_value=10, default_value=0)\n",
    "fill_missing_values(test_data, 'employee_count', unemployed_value=7, default_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(data, drop_columns):\n",
    "    #simplify JOB_42?\n",
    "    data[\"JOB_42_simple\"] = data[\"JOB_42\"].str.extract(r'csp_(\\d+)_\\d+')[0].astype(\"Int64\")\n",
    "\n",
    "    # Fill missing Pay and WORKING_HOURS for unemployed\n",
    "    data.loc[(data['emp_contract'] == 'Unemployed') & (data['Pay'].isna()), 'Pay'] = 0\n",
    "    data.loc[(data['emp_contract'] == 'Unemployed') & (data['WORKING_HOURS'].isna()), 'WORKING_HOURS'] = 0\n",
    "    \n",
    "    return data.drop(columns=drop_columns)\n",
    "\n",
    "\n",
    "# Define columns to drop for each dataset\n",
    "drop_columns = [\n",
    "    \"act\", \"former_emp_contract\", \"RETIREMENT_PAY\", \n",
    "    \"Economic_sector\", \"Code\", \"Libellé\", \"municipality_type\",\n",
    "    \"Nomenclature\", \"X\", \"Y\", \"INSEE_CODE\", \"Nom de la commune\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_data = clean_data(learn_data, drop_columns)\n",
    "test_data = clean_data(test_data, drop_columns)\n",
    "\n",
    "#make sure to drop one of the JOB_42 cols later - simplified or not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Investigate reason for missing values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_values_table(df):\n",
    "        mis_val = df.isnull().sum()\n",
    "        mis_val_percent = 100 * df.isnull().sum() / len(df)  #% of missing values\n",
    "        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)  #create result table\n",
    "        mis_val_table_ren_columns = mis_val_table.rename(\n",
    "        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n",
    "        mis_val_table_ren_columns = mis_val_table_ren_columns[\n",
    "            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n",
    "        '% of Total Values', ascending=False).round(1)\n",
    "        print (\"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n",
    "              \" columns that have missing values.\")\n",
    "        return mis_val_table_ren_columns\n",
    "\n",
    "\n",
    "\n",
    "# Convert the missing values table to LaTeX format\n",
    "#latex_learn_data = missing_values_learn_data.to_latex(index=False, float_format=\"%.2f\")\n",
    "#latex_test_data = missing_values_test_data.to_latex(index=False, float_format=\"%.2f\")\n",
    "\n",
    "#with open(\"./missing_values_learn_data.tex\", \"w\") as file:\n",
    "#    file.write(latex_learn_data)\n",
    "#with open(\"./missing_values_test_data.tex\", \"w\") as file:\n",
    "#    file.write(latex_test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(missing_values_table(learn_data)) \n",
    "print(missing_values_table(test_data)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_na_correlation(df):\n",
    "    \"\"\"\n",
    "    Plots the correlation matrix of missing values in a DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The DataFrame with missing values.\n",
    "    \"\"\"\n",
    "    # Calculate correlation of missing values\n",
    "    missing_corr = df.isnull().corr()\n",
    "\n",
    "    # Mask to hide the upper triangle\n",
    "    mask = np.triu(np.ones_like(missing_corr, dtype=bool))\n",
    "\n",
    "    # Plot the heatmap\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(missing_corr, annot=True, fmt='.2f', cmap='coolwarm', mask=mask, cbar=True)\n",
    "    plt.title(\"Correlation of Missing Values\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Correlation of Missing Values for Learn Data\")\n",
    "plot_na_correlation(learn_data)\n",
    "\n",
    "print(\"Correlation of Missing Values for Test Data\")\n",
    "plot_na_correlation(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nb_na_values_per_rows(df):\n",
    "    \"\"\"\n",
    "    Group rows with exactly X missing values by JOB_42 categories \n",
    "    \"\"\"\n",
    "\n",
    "    # Count the number of missing values per row\n",
    "    df['missing_count'] = df.isnull().sum(axis=1)\n",
    "\n",
    "    # Filter rows with exactly 1 missing value\n",
    "    filtered_df = df[df['missing_count'] == 1].copy() #adjust accordingly\n",
    "\n",
    "    # Identify all columns with missing values for each row\n",
    "    filtered_df['Missing_Variables'] = filtered_df.apply(\n",
    "        lambda row: ', '.join(row.index[row.isnull()].tolist()), axis=1\n",
    "    )\n",
    "\n",
    "    # Group by JOB_42 and calculate counts of rows with 1 missing value\n",
    "    job_42_counts = filtered_df.groupby('JOB_42').size().reset_index(name='Count')\n",
    "\n",
    "    # Calculate total individuals per JOB_42\n",
    "    total_counts = df['JOB_42'].value_counts().reset_index()\n",
    "    total_counts.columns = ['JOB_42', 'Total']\n",
    "\n",
    "    # Merge with filtered counts to calculate percentages\n",
    "    job_42_summary = pd.merge(job_42_counts, total_counts, on='JOB_42')\n",
    "    job_42_summary['Percentage'] = ((job_42_summary['Count'] / job_42_summary['Total']) * 100).round(1)\n",
    "\n",
    "    # Add a list of all missing variables per JOB_42 category\n",
    "    missing_variable_summary = (\n",
    "        filtered_df.groupby('JOB_42')['Missing_Variables']\n",
    "        .apply(lambda x: ', '.join(x.unique()))\n",
    "        .reset_index()\n",
    "    )\n",
    "    missing_variable_summary.columns = ['JOB_42', 'Missing_Variables']\n",
    "\n",
    "    # Merge the summary with the missing variable details\n",
    "    result = pd.merge(job_42_summary, missing_variable_summary, on='JOB_42')\n",
    "\n",
    "    # Drop the temporary column\n",
    "    df.drop(columns=['missing_count'], inplace=True)\n",
    "\n",
    "    return result.sort_values(by='Percentage', ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_values = nb_na_values_per_rows(learn_data)\n",
    "nb_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def na_values_by_group_for_column(df, group_col, target_col):\n",
    "    # Group by `group_col` and calculate missing values for `target_col`\n",
    "    grouped = df.groupby(group_col).apply(lambda group: group[target_col].isnull().sum())\n",
    "    total_rows = df.groupby(group_col).size()  # Total rows per group\n",
    "    missing_percent = (grouped / total_rows) * 100  # Calculate % of missing values\n",
    "\n",
    "    # Combine counts and percentages into one DataFrame\n",
    "    missing_table = pd.DataFrame({\n",
    "        'Missing Values': grouped,\n",
    "        '% of Total Values': missing_percent\n",
    "    }).round(1)\n",
    "    \n",
    "    return missing_table"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "missing_job_reg_by_household = na_values_by_group_for_column(learn_data, \"JOB_42\", \"Employer_category\")\n",
    "missing_job_reg_by_household"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enconding the dataset to ease imputation and use of ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for linear regression (MICE, KNN) I need only number :\n",
    "\n",
    "#do one-hot encoding for \"labor_force status\", but with catagegory being 0(=??), Employed_missing, Unemployed and Other (=only being 6%)\n",
    "#do one hot-encodung for \"type_of_contratct\" with CDI, Unemployed, Employed_Missing, Other (=only being 12%)\n",
    "#do one_hot_encding for \"work_condition\" with Unemployed, P, Other(=only being 17%)\n",
    "#do one hot_econding for \"emp_contract\", with EMP1-6, Unemployed, EMP2-1, Other (=being 12%)\n",
    "#do one hot encoding for \"sport\" but with a \"Other\" category\n",
    "#do one hot encdoding for \"employer_category\" with 1 to 7 being \"Others\"\n",
    "#for \"act\" do one hot encoding but maybe merge stay at home people with inactif + drop less than 14years olds as nobody in our data is \n",
    "\n",
    "#use \"work condition\" instead of JOB_42 and then do one-hot-encdoding\n",
    "#do one-hot-encoding for \"household_type\"\n",
    "#do one hot encoding for \"Economic_sector_num\" and \"work_description\" \n",
    "#do one hot encdoding for \"REG_JOB\" and \"FORMER_REG\"\n",
    "\n",
    "#can keep it as is for \"employee count\" because here the ordinality make sense\n",
    "#can keep as is for \"highest_credential\" because the ordinality make sense here for education\n",
    "#can keep as is for the location of insee code because here the ordinality make sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_dataset(df):\n",
    "    \"\"\"\n",
    "    Encodes a dataset based on a custom encoding plan.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Drop rows with age < 14\n",
    "    df = df[df['age_2020'] >= 14]\n",
    "\n",
    "    # One-hot encoding with custom categories\n",
    "    def custom_one_hot(column, categories, other_name='Other'):\n",
    "        df[column] = df[column].apply(lambda x: x if x in categories else other_name)\n",
    "        return pd.get_dummies(df[column], prefix=column)\n",
    "\n",
    "    # Encoding labor_force_status\n",
    "    df_labor_force = custom_one_hot('labor_force_status', \n",
    "                                    ['0', 'Employed_missing', 'Unemployed', 'Other']).astype(int)\n",
    "\n",
    "    # Encoding type_of_contract\n",
    "    df_contract = custom_one_hot('TYPE_OF_CONTRACT', \n",
    "                                  ['CDI', 'Unemployed', 'Employed_missing', 'Other']).astype(int)\n",
    "\n",
    "    # Encoding work_condition\n",
    "    df_condition = custom_one_hot('WORK_CONDITION', \n",
    "                                   ['Unemployed', 'P', 'Other']).astype(int)\n",
    "\n",
    "    # Encoding emp_contract\n",
    "    df_emp_contract = custom_one_hot('emp_contract', \n",
    "                                      ['EMP1-6', 'Unemployed', 'EMP2-1', 'Other']).astype(int)\n",
    "\n",
    "    # Encoding sport\n",
    "    df_sport = custom_one_hot('Sports_Category', df['Sports_Category'].unique(), other_name='Other').astype(int)\n",
    "\n",
    "    # Encoding retirement_age with one-hot encoding for the categories: 'Employed', 'Unemployed', and other categorie\n",
    "    df_retirement_age = pd.get_dummies(df['retirement_age'], prefix='retirement_age', dummy_na=True).astype(int)\n",
    "\n",
    "# You can now replace the previous three separate columns with this new encoding\n",
    "\n",
    "    # Encoding employer_category (1-7 as \"Other\")\n",
    "    df['Employer_category'] = df['Employer_category'].apply(lambda x: x if x not in [1, 2, 3, 4, 5, 6, 7] else 'Other')\n",
    "    df_employer = pd.get_dummies(df['Employer_category'], prefix='Employer_category').astype(int)\n",
    "\n",
    "    # Encoding act (merge categories)\n",
    "    df['act_num'] = df['act_num'].apply(lambda x: 'Inactive' if x in ['Stay_at_home', 'Inactive'] else x)\n",
    "    df_act = pd.get_dummies(df['act_num'], prefix='act').astype(int)\n",
    "\n",
    "    df_JOB_42 = pd.get_dummies(df['JOB_42'], prefix='JOB_42').astype(int)\n",
    "\n",
    "    df_FORMER_JOB_42 = pd.get_dummies(df['FORMER_JOB_42'], prefix='JOB_42').astype(int)\n",
    "    \n",
    "    df_work_description = pd.get_dummies(df['work_description'], prefix='work_description').astype(int)\n",
    "\n",
    "\n",
    "    # Encoding work_condition (replacing JOB_42)\n",
    "    #df_work_condition = custom_one_hot('WORK_CONDITION', df['WORK_CONDITION'].unique())\n",
    "\n",
    "    # Encoding household_type\n",
    "    df_household = pd.get_dummies(df['HOUSEHOLD_TYPE'], prefix='HOUSEHOLD_TYPE').astype(int)\n",
    "\n",
    "    # Encoding Economic_sector_num and work_description\n",
    "    df_sector = pd.get_dummies(df['Economic_sector_num'], prefix='Economic_sector_num').astype(int)\n",
    "    \n",
    "    # Encoding REG_JOB and FORMER_REG\n",
    "    df_reg_job = pd.get_dummies(df['JOB_REG'], prefix='JOB_REG').astype(int)\n",
    "    df_former_reg = pd.get_dummies(df['FORMER_REG'], prefix='FORMER_REG').astype(int)\n",
    "    df_current_reg = pd.get_dummies(df['CURRENT_REG'], prefix='CURRENT_REG').astype(int)\n",
    "\n",
    "    # Combine all encoded columns with unchanged ordinal columns\n",
    "    encoded_df = pd.concat([\n",
    "        df[[\"PRIMARY_KEY\", 'sex','studying','age_2020', \"target\", 'employee_count', 'HIGHEST_CREDENTIAL_num', \"Lat\", \"long\"]],  # Ordinal columns\n",
    "        df_retirement_age, df_labor_force, df_contract, df_condition, df_emp_contract, df_sport,\n",
    "        df_employer, df_act, df_household, df_sector, df_work_description, df_reg_job, df_former_reg, \n",
    "        df_JOB_42, df_FORMER_JOB_42, df_current_reg\n",
    "    ], axis=1)\n",
    "\n",
    "    return encoded_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = encode_dataset(learn_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling NA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn_data[\"work_description\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1547,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_factorize = [  \n",
    "    'JOB_42', 'JOB_REG', 'Economic_sector_num', 'work_description', 'employee_count',\n",
    "    'retirement_age_cat', 'FORMER_REG', 'TYPE_OF_CONTRACT', 'FORMER_JOB_42',\n",
    "    'WORK_CONDITION', 'labor_force_status', 'emp_contract', 'Employer_category'\n",
    "]  \n",
    "\n",
    "# Function to sort and factorize columns while keeping missing values as NaN\n",
    "def sort_and_factorize_with_na(data, columns):\n",
    "    for column in columns:\n",
    "        # Sort by the column while keeping NaN values intact\n",
    "        data = data.sort_values(by=column, ascending=True, na_position='first')\n",
    "        \n",
    "        # Get factorized codes, skipping NaN values\n",
    "        codes, uniques = pd.factorize(data[column], use_na_sentinel=True)\n",
    "        \n",
    "        # Assign codes to the column, replacing -1 (representing NaN) back with NaN\n",
    "        data[column] = pd.Series(codes, index=data.index).replace(-1, pd.NA)\n",
    "        data[column] = pd.to_numeric(data[column], errors='coerce').astype('category')\n",
    "        \n",
    "    return data.sort_values(by='PRIMARY_KEY', ascending=True)\n",
    "\n",
    "# Apply the function to learn_data and test_data\n",
    "learn_data = sort_and_factorize_with_na(learn_data, columns_to_factorize)\n",
    "test_data = sort_and_factorize_with_na(test_data, columns_to_factorize)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imputing NA for rows with only 1 NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_data.dtypes"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#SO THIS IS NOT NEEDED ANYMOOORE?????? -- double check!!!!!!!!!\n",
    "\n",
    "def impute_missing_values_by_job_42(df):\n",
    "    \"\"\"\n",
    "    Imputes missing values by median within each JOB_42 category for rows with exactly 1 missing value.\n",
    "    \"\"\"\n",
    "    # Count the number of missing values per row\n",
    "    df['missing_count'] = df.isnull().sum(axis=1)\n",
    "\n",
    "    # Filter rows with exactly 1 missing value\n",
    "    filtered_df = df[df['missing_count'] == 1]\n",
    "\n",
    "    # Iterate through rows with 1 missing value\n",
    "    for index, row in filtered_df.iterrows():\n",
    "        # Find the missing column\n",
    "        missing_col = row[row.isnull()].index[0]\n",
    "\n",
    "        # Get the JOB_42 category\n",
    "        job_42_category = row['JOB_42']\n",
    "\n",
    "        # Calculate the median of the missing column for the specific JOB_42 category\n",
    "        median_value = df[df['JOB_42'] == job_42_category][missing_col].median()\n",
    "\n",
    "        # Impute the missing value with the calculated median\n",
    "        df.at[index, missing_col] = median_value\n",
    "\n",
    "    # Drop the temporary column\n",
    "    df.drop(columns=['missing_count'], inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_data[\"Employer_category\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## THIS ISNT WORKING YET SOB\n",
    "def impute_employer_category_with_1_missing(data):\n",
    "    # Add a column to count missing values specifically for 'Employer_category' column\n",
    "    data['missing_count'] = data['Employer_category'].isnull().astype(int)\n",
    "\n",
    "    # Filter rows where 'Employer_category' has exactly 1 missing value\n",
    "    rows_with_1_missing = data[data['missing_count'] == 1]\n",
    "\n",
    "    # Create a copy of the rows with 1 missing being 'Employer_category' for encoding\n",
    "    encoded_df = rows_with_1_missing.copy()\n",
    "\n",
    "    # Apply KNN Imputation only to the 'Employer_category' column\n",
    "    knn_imputer = KNNImputer(n_neighbors=5)\n",
    "    imputed_array = knn_imputer.fit_transform(encoded_df[['Employer_category']])\n",
    "\n",
    "    # Recreate the DataFrame from imputed data\n",
    "    imputed_df = pd.DataFrame(imputed_array, columns=['Employer_category'], index=encoded_df.index)\n",
    "\n",
    "    # Convert the imputed 'Employer_category' column back to its original category type\n",
    "    if isinstance(data['Employer_category'].dtype, pd.CategoricalDtype):  # Check if the column is categorical\n",
    "        imputed_df['Employer_category'] = pd.Categorical.from_codes(\n",
    "            imputed_df['Employer_category'].round().astype(int),\n",
    "            categories=data['Employer_category'].cat.categories\n",
    "        )\n",
    "\n",
    "    # Replace the imputed 'Employer_category' values in the original dataset\n",
    "    data.loc[rows_with_1_missing.index, 'Employer_category'] = imputed_df['Employer_category']\n",
    "\n",
    "    # Drop the temporary 'missing_count' column\n",
    "    data.drop(columns=['missing_count'], inplace=True)\n",
    "\n",
    "    return data\n",
    "\n",
    "# Apply the function to learn_data\n",
    "learn_data = impute_employer_category_with_1_missing(learn_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#impute_missing_values_by_job_42(learn_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imputing NA for rows with 2 NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn_data[\"missing_count\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1551,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_rows_with_2_missing(data):\n",
    "    # Add a column to count missing values per row\n",
    "    data['missing_count'] = data.isnull().sum(axis=1)\n",
    "\n",
    "    # Filter rows with exactly 2 missing values\n",
    "    rows_with_2_missing = data[data['missing_count'] == 2].drop(columns='missing_count')\n",
    "\n",
    "    # Create a copy for encoding\n",
    "    encoded_df = rows_with_2_missing.copy()\n",
    "\n",
    "    # Apply KNN Imputation\n",
    "    knn_imputer = KNNImputer(n_neighbors=5)\n",
    "    imputed_array = knn_imputer.fit_transform(encoded_df)\n",
    "\n",
    "    # Recreate the DataFrame from imputed data\n",
    "    imputed_df = pd.DataFrame(imputed_array, columns=encoded_df.columns, index=encoded_df.index)\n",
    "\n",
    "    # Convert the imputed columns back to their original category types\n",
    "    for col in imputed_df.columns:\n",
    "        if isinstance(data[col].dtype, pd.CategoricalDtype):  # Updated dtype check\n",
    "            # Map numeric codes back to original categories\n",
    "            imputed_df[col] = pd.Categorical.from_codes(\n",
    "                imputed_df[col].round().astype(int), \n",
    "                categories=data[col].cat.categories\n",
    "            )\n",
    "        else:\n",
    "            # Keep non-categorical columns as they are\n",
    "            imputed_df[col] = imputed_df[col]\n",
    "\n",
    "    # Replace the imputed rows in the original dataset\n",
    "    data.loc[rows_with_2_missing.index, :] = imputed_df\n",
    "\n",
    "    # Drop the temporary column used for filtering\n",
    "    data.drop(columns=['missing_count'], inplace=True)\n",
    "\n",
    "    return data\n",
    "\n",
    "# Apply the function to learn_data\n",
    "learn_data = impute_rows_with_2_missing(learn_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replacing NA with external values (for pay and working hours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1554,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add section here to replace missing JOB_REG with CURRENT_REG for some JOB_42 categories\n",
    "# - eg artisans, agriculture...!!!!!!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1556,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define job-specific pay and working hours\n",
    "job_defaults = {\n",
    "    'csp_1': {'Pay': 50000, 'WORKING_HOURS': 2860},  # Agriculture\n",
    "    'csp_2_1': {'Pay': 24000, 'WORKING_HOURS': 2288},  # Artisans\n",
    "    'csp_2_2': {'Pay': 39937, 'WORKING_HOURS': 2444},  # Commerçant\n",
    "    'csp_2_3': {'Pay': 58248, 'WORKING_HOURS': 2704},  # Chefs d'enterprise\n",
    "}\n",
    "\n",
    "# Function to fill missing values for Pay and WORKING_HOURS\n",
    "def fill_job_defaults(data, job_defaults):\n",
    "    for job, defaults in job_defaults.items():\n",
    "        if job == 'csp_1':\n",
    "            # Special case: Handle 'startswith' condition for 'csp_1'\n",
    "            job_condition = data['JOB_42'].astype(str).str.startswith(job)\n",
    "        else:\n",
    "            # Exact match for other job keys\n",
    "            job_condition = data['JOB_42'] == job\n",
    "        \n",
    "        # Fill missing Pay and WORKING_HOURS\n",
    "        data.loc[job_condition & data['Pay'].isna(), 'Pay'] = defaults['Pay']\n",
    "        data.loc[job_condition & data['WORKING_HOURS'].isna(), 'WORKING_HOURS'] = defaults['WORKING_HOURS']\n",
    "\n",
    "# Apply the function to the datasets\n",
    "fill_job_defaults(learn_data, job_defaults)\n",
    "fill_job_defaults(test_data, job_defaults)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(missing_values_table(learn_data)) \n",
    "print(missing_values_table(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values_learn_data = missing_values_table(learn_data)\n",
    "missing_values_test_data = missing_values_table(test_data)\n",
    "\n",
    "# Convert the missing values table to LaTeX format\n",
    "latex_learn_data = missing_values_learn_data.to_latex(float_format=\"%.2f\")\n",
    "latex_test_data = missing_values_test_data.to_latex(float_format=\"%.2f\")\n",
    "\n",
    "with open(\"./missing_values_learn_data.tex\", \"w\") as file:\n",
    "    file.write(latex_learn_data)\n",
    "with open(\"./missing_values_test_data.tex\", \"w\") as file:\n",
    "    file.write(latex_test_data) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sorting for numeric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the pipeline"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Custom Transformer for Handling Imputation with Averages\n",
    "class ImputeMissingValues(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        # Calculate averages for non-missing, non-zero values\n",
    "        self.avg_working_hours = (\n",
    "            X.loc[X[\"WORKING_HOURS\"].notna() & (X[\"WORKING_HOURS\"] != 0)]\n",
    "            .groupby(\"JOB_42\")[\"WORKING_HOURS\"]\n",
    "            .mean()\n",
    "            .round()\n",
    "        )\n",
    "        self.avg_pay = (\n",
    "            X.loc[X[\"Pay\"].notna() & (X[\"Pay\"] != 0)]\n",
    "            .groupby(\"JOB_42\")[\"Pay\"]\n",
    "            .mean()\n",
    "            .round()\n",
    "        )\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        # Make a copy of X to avoid modifying the original dataframe\n",
    "        X = X.copy()\n",
    "        \n",
    "        # Impute WORKING_HOURS using averages\n",
    "        X[\"WORKING_HOURS\"] = X.apply(\n",
    "            lambda row: self.avg_working_hours.get(row[\"JOB_42\"], np.nan)\n",
    "            if pd.isnull(row[\"WORKING_HOURS\"]) else row[\"WORKING_HOURS\"],\n",
    "            axis=1\n",
    "        )\n",
    "        \n",
    "        # Impute Pay using averages\n",
    "        X[\"Pay\"] = X.apply(\n",
    "            lambda row: self.avg_pay.get(row[\"JOB_42\"], np.nan)\n",
    "            if pd.isnull(row[\"Pay\"]) else row[\"Pay\"],\n",
    "            axis=1\n",
    "        )\n",
    "        \n",
    "        return X\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImputeMissingValuesByJob42(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Scikit-learn compatible transformer to impute missing values by median within each JOB_42 category\n",
    "    for rows with exactly 1 missing value.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Prepares the transformer by storing the necessary medians for imputation.\n",
    "\n",
    "        Parameters:\n",
    "            X (pd.DataFrame): The DataFrame to analyze.\n",
    "            y (Optional): Ignored. For compatibility with scikit-learn pipelines.\n",
    "\n",
    "        Returns:\n",
    "            self: Fitted transformer.\n",
    "        \"\"\"\n",
    "        # Identify columns with missing values and group medians by JOB_42\n",
    "        self.medians_by_category = {\n",
    "            column: X.groupby('JOB_42')[column].median()\n",
    "            for column in X.columns\n",
    "            if X[column].isnull().any() and column != 'JOB_42'\n",
    "        }\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Transforms the DataFrame by imputing missing values with the median within JOB_42 categories.\n",
    "\n",
    "        Parameters:\n",
    "            X (pd.DataFrame): The DataFrame to transform.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Transformed DataFrame with missing values imputed.\n",
    "        \"\"\"\n",
    "        X = X.copy()\n",
    "        \n",
    "        # Count the number of missing values per row\n",
    "        X['missing_count'] = X.isnull().sum(axis=1)\n",
    "\n",
    "        # Filter rows with exactly 1 missing value\n",
    "        filtered_df = X[X['missing_count'] == 1]\n",
    "\n",
    "        # Iterate through rows with 1 missing value\n",
    "        for index, row in filtered_df.iterrows():\n",
    "            # Find the missing column\n",
    "            missing_col = row[row.isnull()].index[0]\n",
    "\n",
    "            # Get the JOB_42 category\n",
    "            job_42_category = row['JOB_42']\n",
    "\n",
    "            # Impute the missing value with the median for the specific JOB_42 category\n",
    "            median_value = self.medians_by_category.get(missing_col, {}).get(job_42_category, None)\n",
    "            X.at[index, missing_col] = median_value\n",
    "\n",
    "        # Drop the temporary column\n",
    "        X.drop(columns=['missing_count'], inplace=True)\n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def mice_imputation_for_categorical(data, columns_to_impute):\n",
    "    \"\"\"\n",
    "    This function imputes missing values for categorical variables using MICE.\n",
    "    \n",
    "    :param data: pandas DataFrame, dataset with missing values.\n",
    "    :param columns_to_impute: list, columns to apply MICE imputation.\n",
    "    \n",
    "    :return: pandas DataFrame, dataset with imputed values.\n",
    "    \"\"\"\n",
    "    # Step 1: Ensure all columns are in numeric format (Label Encoding if necessary)\n",
    "    data_encoded = data.copy()\n",
    " \n",
    "    # Step 2: Apply MICE using IterativeImputer\n",
    "    mice_imputer = IterativeImputer(max_iter=10, random_state=42)\n",
    "    imputed_data = mice_imputer.fit_transform(data_encoded[columns_to_impute])\n",
    "    \n",
    "    # Step 3: Convert the imputed data back to a DataFrame\n",
    "    imputed_data_df = pd.DataFrame(imputed_data, columns=columns_to_impute)\n",
    "    \n",
    "    # Step 4: Revert the encoding for categorical columns\n",
    "    for col, original_col in zip(columns_to_impute, columns_to_impute):\n",
    "        if data[original_col].dtype == 'object':  # Only reverse Label Encoding if necessary\n",
    "            label_encoder = LabelEncoder()\n",
    "            label_encoder.fit(data[original_col].astype(str))\n",
    "            imputed_data_df[original_col] = label_encoder.inverse_transform(imputed_data_df[original_col].astype(int))\n",
    "    \n",
    "    # Step 5: Combine the imputed data with the rest of the original dataset\n",
    "    data_imputed = data.copy()\n",
    "    data_imputed[columns_to_impute] = imputed_data_df[columns_to_impute]\n",
    "    \n",
    "    return data_imputed\n",
    "\n",
    "# Example usage\n",
    "columns_to_impute = ['TYPE_OF_CONTRACT', 'WORK_CONDITION_0.0', 'WORK_CONDITION_1.0', 'WORK_CONDITION_2.0', 'WORK_CONDITION_3.0', 'WORK_CONDITION_4.0', \n",
    "                     'labor_force_status_0.0', 'labor_force_status_1.0', 'labor_force_status_2.0', 'labor_force_status_3.0', \n",
    "                     'labor_force_status_4.0', 'Economic_sector_num', 'work_description']\n",
    "\n",
    "TYPE_OF_CONTRACT               6939               13.9\n",
    "Economic_sector_num            6939               13.9\n",
    "work_description               6939               13.9\n",
    "FORMER_REG                     2097                4.2\n",
    "JOB_REG\n",
    "\n",
    "imputed_data = mice_imputation_for_categorical(learn_data, columns_to_impute)\n",
    "\n",
    "# Display imputed data\n",
    "print(imputed_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mice_imputation_for_categorical(data, columns_to_impute): #works now I think\n",
    "    \"\"\"\n",
    "    This function imputes missing values for categorical variables using MICE.\n",
    "    \n",
    "    :param data: pandas DataFrame, dataset with missing values.\n",
    "    :param columns_to_impute: list, columns to apply MICE imputation.\n",
    "    \n",
    "    :return: pandas DataFrame, dataset with imputed values.\n",
    "    \"\"\"\n",
    "    # Step 1: Ensure all columns are in numeric format (Label Encoding if necessary)\n",
    "    data_encoded = data.copy()\n",
    " \n",
    "    # Step 2: Apply MICE using IterativeImputer\n",
    "    mice_imputer = IterativeImputer(max_iter=10, random_state=42)\n",
    "    imputed_data = mice_imputer.fit_transform(data_encoded[columns_to_impute])\n",
    "    \n",
    "    # Step 3: Convert the imputed data back to a DataFrame\n",
    "    imputed_data_df = pd.DataFrame(imputed_data, columns=columns_to_impute)\n",
    "\n",
    "    # Step 5: Combine the imputed data with the rest of the original dataset\n",
    "    data_imputed = data.copy()\n",
    "    data_imputed[columns_to_impute] = imputed_data_df[columns_to_impute]\n",
    "    \n",
    "    return data_imputed\n",
    "\n",
    "# Example usage\n",
    "columns_to_impute = ['TYPE_OF_CONTRACT', 'WORK_CONDITION', \n",
    "                     'labor_force_status', 'Economic_sector_num', 'work_description']\n",
    "\n",
    "imputed_data = mice_imputation_for_categorical(learn_data, columns_to_impute)\n",
    "\n",
    "# Display imputed data\n",
    "print(imputed_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(missing_values_table(learn_data)) \n",
    "print(missing_values_table(imputed_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NEED TO BE MODIFIED - GET WORKING FUNCTION FIRST\n",
    "\n",
    "# Specify the columns that need MICE imputation\n",
    "cat_columns = [\"TYPE_OF_CONTRACT\", \"WORK_CONDITION\", \"labor_force_status\", \"work_description\", 'Economic_sector_num']\n",
    "columns_to_impute = [\"Employer_category\", \"employee_count\"]\n",
    "\n",
    "# Define the combined pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('job_42_imputer', ImputeMissingValuesByJob42()) #- for when theres 1 missing value/col per row \n",
    "    #('impute_averages', ImputeMissingValues()), #this one was for working hours and pay specifically?\n",
    "    ('mice_imputer', MICEImputer(columns_to_impute=columns_to_impute)), \n",
    "    ('knn_imputer', Custom_KNNImputer(columns_to_impute_2=columns_to_impute_2)),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LINEAR MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the encoding is done using the previous function (encoding_datataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_values_table(df):\n",
    "        mis_val = df.isnull().sum()\n",
    "        mis_val_percent = 100 * df.isnull().sum() / len(df)  #% of missing values\n",
    "        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)  #create result table\n",
    "        mis_val_table_ren_columns = mis_val_table.rename(\n",
    "        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n",
    "        mis_val_table_ren_columns = mis_val_table_ren_columns[\n",
    "            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n",
    "        '% of Total Values', ascending=False).round(1)\n",
    "        print (\"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n",
    "              \" columns that have missing values.\")\n",
    "        return mis_val_table_ren_columns\n",
    "\n",
    "print(missing_values_table(learn_lin)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#move this for after train test splitting\n",
    "learn_lin = pipeline.fit_transform(learn_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_lin = learn_lin.drop(columns=[\"target\"])\n",
    "y_train_lin = learn_lin[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_lin, X_test_lin, Y_train_lin, Y_test_lin = train_test_split(x_train_lin,\n",
    "                                                    y_train_lin, \n",
    "                                                    test_size=0.3, \n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example GridSearchCV for Linear Regression\n",
    "param_grid = {'fit_intercept': [True, False]}  # Define your hyperparameter grid\n",
    "\n",
    "# Create a Linear Regression model\n",
    "lr = LinearRegression()\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "lin_res = GridSearchCV(estimator=lr, param_grid=param_grid, cv=5)\n",
    "\n",
    "# Fit the GridSearchCV model\n",
    "lin_res.fit(X_train_lin, Y_train_lin)\n",
    "\n",
    "# Output the best model and its score\n",
    "print(f\"Best model: {lin_res.best_estimator_}\")\n",
    "print(f\"Best score: {lin_res.best_score_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions = lin_res.predict(X_train_lin)\n",
    "print(\"R² for training set: \", r2_score(Y_train_lin, train_predictions))\n",
    "print(\"RMSE on the learning set:\", root_mean_squared_error(Y_train_lin, lin_res.predict(X_train_lin)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions = lin_res.predict(X_train_lin)\n",
    "test_predictions = lin_res.predict(X_test_lin)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x=Y_test_lin, y=test_predictions, alpha=0.6, color=\"blue\")\n",
    "plt.plot([y_train_lin.min(), y_train_lin.max()], [y_train_lin.min(), y_train_lin.max()], \"--r\", linewidth=2)  # Ideal predictions\n",
    "plt.xlabel('True Target Value')\n",
    "plt.ylabel('Predicted Target Value')\n",
    "plt.title('Prediction vs True Values on Test Set')\n",
    "plt.savefig(\"./Prediction_vs_True_linear.png\", format='png', dpi=300)  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RANDOM FOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_rf = pipeline.fit_transform(learn_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_rf = learn_rf.drop(columns=[\"target\"])\n",
    "y_train_rf = learn_rf[\"target\"]\n",
    "\n",
    "X_train_rf, X_test_rf, Y_train_rf, Y_test_rf = train_test_split(x_train_rf,\n",
    "                                                    y_train_rf, \n",
    "                                                    test_size=0.3, \n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_rf = {     # Number of trees in the forest\n",
    "    'max_depth': [5, 10, 30, 50],    \n",
    "    'min_samples_split': [5, 10, 20]  \n",
    "}\n",
    "\n",
    "# Define cross-validation with 5 folds\n",
    "cv_folds = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize the RandomForestRegressor model\n",
    "rf = RandomForestRegressor(random_state=42)  #need to add that in the pipeline \n",
    "\n",
    "# Set up the GridSearchCV for RandomForestRegressor with appropriate scoring metric\n",
    "rf_search = GridSearchCV(rf, param_grid_rf, cv=cv_folds, n_jobs=-1, scoring=\"neg_mean_squared_error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model on the training data\n",
    "rf_res = rf_search.fit(X_train_rf, Y_train_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best model after GridSearchCV\n",
    "best_model_rf = rf_res.best_estimator_\n",
    "\n",
    "# Output the best model and its score\n",
    "print(f\"Best model: {best_model_rf}\")\n",
    "print(f\"Best score: {rf_res.best_score_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_scores = cross_val_score(best_model_rf, X_train_rf, Y_train_rf, cv=5, scoring='r2')\n",
    "print(f\"Cross-validated R2 scores: {cv_scores}\")\n",
    "print(f\"Average R2: {np.mean(cv_scores):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions = best_model_rf.predict(X_train_rf)\n",
    "test_predictions = best_model_rf.predict(X_test_rf)\n",
    "\n",
    "print(\"R² for training set: \", r2_score(Y_train_rf, train_predictions))\n",
    "print(\"R² for test set: \", r2_score(Y_test_rf, test_predictions))\n",
    "print(\"RMSE on the learning set:\", root_mean_squared_error(Y_train_rf, best_model_rf.predict(X_train_rf)))\n",
    "print(\"RMSE on the test set:\", root_mean_squared_error(Y_test_rf, best_model_rf.predict(X_test_rf)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_metrics = pd.DataFrame({\n",
    "    'Metric': ['R²', 'RMSE'],\n",
    "    'Train': [r2_score(Y_train_rf, train_predictions), root_mean_squared_error(Y_train_rf, best_model_rf.predict(X_train_rf))],\n",
    "    'Test': [r2_score(Y_test_rf, test_predictions), root_mean_squared_error(Y_test_rf, best_model_rf.predict(X_test_rf))]\n",
    "})\n",
    "\n",
    "# Set 'Metric' as the index to have a clean format\n",
    "error_metrics.set_index('Metric', inplace=True)\n",
    "\n",
    "# Convert to LaTeX format\n",
    "error_metrics_table = error_metrics.to_latex(index=True, float_format=\"%.2f\")\n",
    "\n",
    "with open(\"./error_metrics.tex\", \"w\") as file:\n",
    "    file.write(error_metrics_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x=Y_test_rf, y=test_predictions, alpha=0.6, color=\"blue\")\n",
    "plt.plot([y_train_rf.min(), y_train_rf.max()], [y_train_rf.min(), y_train_rf.max()], \"--r\", linewidth=2)  # Ideal predictions\n",
    "plt.xlabel('True Target Value')\n",
    "plt.ylabel('Predicted Target Value')\n",
    "plt.title('Prediction vs True Values on Test Set')\n",
    "plt.savefig(\"./Prediction_vs_True_rf.png\", format='png', dpi=300)  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXTRA ANALYISIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation\n",
    "correlation_matrix = learn_data.corr()\n",
    "\n",
    "# Find correlations with the target variable\n",
    "target_corr = correlation_matrix['target'].sort_values(ascending=False)\n",
    "\n",
    "print(target_corr)\n",
    "\n",
    "target_corr_table = target_corr.to_latex(index=False, float_format=\"%.2f\")\n",
    "\n",
    "with open(\"./target_corr.tex\", \"w\") as file:\n",
    "    file.write(target_corr_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances\n",
    "feature_importances = pd.DataFrame({\n",
    "    'Feature': x_train_rf.columns,\n",
    "    'Importance': best_model_rf.feature_importances_\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(feature_importances)\n",
    "\n",
    "feature_importances_table = feature_importances.to_latex(index=False, float_format=\"%.2f\")\n",
    "\n",
    "with open(\"./feature_importances.tex\", \"w\") as file:\n",
    "    file.write(feature_importances_table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
