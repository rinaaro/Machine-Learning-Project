{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split, ShuffleSplit, KFold, cross_val_score, GridSearchCV, StratifiedKFold\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import root_mean_squared_error, r2_score,  mean_squared_error\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import SimpleImputer, KNNImputer, IterativeImputer\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.linear_model import LinearRegression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_name in os.listdir(\"files\"):\n",
    "    if file_name.endswith('.csv'):\n",
    "        file_path = os.path.join(\"files\", file_name)\n",
    "\n",
    "        df_name = os.path.splitext(file_name)[0]\n",
    "        globals()[df_name] = pd.read_csv(file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simplification of categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge sports data and extract sports categories\n",
    "def extract_sports_category(dataset, sports_code):\n",
    "    merged = pd.merge(dataset, sports_code, left_on=\"Sports\", right_on=\"Code\")\n",
    "    merged[\"Sports_Category\"] = merged[\"Categorie\"]\n",
    "    return merged[[\"PRIMARY_KEY\", \"Sports_Category\"]]\n",
    "\n",
    "learn_sports = extract_sports_category(learn_dataset_sport, code_Sports)\n",
    "test_sports = extract_sports_category(test_dataset_sport, code_Sports)\n",
    "\n",
    "# Merge departments into regions and extract relevant region columns\n",
    "def merge_and_extract_region(df, merge_column, region_column_name):\n",
    "    merged = pd.merge(df, departments, left_on=merge_column, right_on=\"DEP\", how=\"left\")\n",
    "    merged[region_column_name] = merged[\"REG\"]\n",
    "    return merged.drop([\"Nom du département\", \"REG\", \"DEP\", merge_column], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_dataset_job = merge_and_extract_region(learn_dataset_job, \"JOB_DEP\", \"JOB_REG\")    #put the column as int \n",
    "learn_dataset_retired_jobs = merge_and_extract_region(learn_dataset_retired_jobs, \"JOB_DEP\", \"JOB_REG\")\n",
    "learn_dataset_retired_jobs = merge_and_extract_region(learn_dataset_retired_jobs, \"FORMER_DEP\", \"FORMER_REG\")\n",
    "\n",
    "test_dataset_job = merge_and_extract_region(test_dataset_job, \"JOB_DEP\", \"JOB_REG\")\n",
    "test_dataset_retired_jobs = merge_and_extract_region(test_dataset_retired_jobs, \"JOB_DEP\", \"JOB_REG\")\n",
    "test_dataset_retired_jobs = merge_and_extract_region(test_dataset_retired_jobs, \"FORMER_DEP\", \"FORMER_REG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Economic sector into fewer categories (and numeric instead of object/string)\n",
    "def sector_mapping(nace_code):\n",
    "    if nace_code == \"AZ\":  \n",
    "        return \"Agriculture, forestry and fishing)\"\n",
    "    elif \"BZ\" <= nace_code <= \"EZ\":\n",
    "        return \"Manufacturing, mining and quarrying and other industrial activities\"\n",
    "    elif nace_code == \"FZ\": \n",
    "        return \"Construction\"\n",
    "    elif \"GZ\" <= nace_code <= \"IZ\":  \n",
    "        return \"Wholesale and retail trade, transportation and storage, accommodation and food service activities\"\n",
    "    elif \"JA\" <= nace_code <= \"JC\":\n",
    "        return \"Information and communication\"\n",
    "    elif nace_code == \"KZ\": \n",
    "        return \"Financial and insurance activities\"\n",
    "    elif nace_code == \"LZ\": \n",
    "        return \"Real estate activities\"\n",
    "    elif \"MA\" <= nace_code <= \"NZ\":\n",
    "        return \"Professional, scientific, technical, administrative and support service activities\"\n",
    "    elif \"OZ\" <= nace_code <= \"QB\":\n",
    "        return \"Public administration and defence, education, human health and social work activities\"\n",
    "    elif \"RZ\" <= nace_code <= \"UZ\":\n",
    "        return \"Other services activities\"\n",
    "    else:\n",
    "        return \"Unknown Sector\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_Economic_sector[\"Nomenclature\"] = code_Economic_sector[\"Code\"].map(sector_mapping)\n",
    "code_Economic_sector[\"Economic_sector_num\"] = pd.factorize(code_Economic_sector[\"Nomenclature\"])[0] + 1\n",
    "\n",
    "#issue in the code IZ should be its own category \"Hébergement et restauration\"\n",
    "#HZ, JZ à MC so LZ, KZ, MA, MB and MC, NZ, OZ, PZ,  QA, QB, RZ, SZ, TZ et UZ should be in the same category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_HIGHEST_CREDENTIAL[\"HIGHEST_CREDENTIAL_num\"] = pd.factorize(code_HIGHEST_CREDENTIAL[\"Code\"])[0] + 1\n",
    "code_act[\"act_num\"] = pd.factorize(code_act[\"Code\"])[0] + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_data = learn_dataset\n",
    "\n",
    "learn_data = pd.merge(learn_data, code_act, left_on=\"act\", right_on=\"Code\", how=\"left\")\n",
    "learn_data.drop([\"Code\", \"Libellé\"], axis=1, inplace=True)  #should also drop act ?\n",
    "learn_data = pd.merge(learn_data, code_HIGHEST_CREDENTIAL, left_on=\"HIGHEST_CREDENTIAL\", right_on=\"Code\", how=\"left\")\n",
    "learn_data.drop([\"Code\", \"Libellé\", \"HIGHEST_CREDENTIAL\"], axis=1, inplace=True)\n",
    "\n",
    "#for imputation fitting\n",
    "learn_data = pd.merge(learn_data, city_pop, on=\"INSEE_CODE\", how=\"left\")\n",
    "learn_data = pd.merge(learn_data, city_loc, on=\"INSEE_CODE\", how=\"left\")\n",
    "\n",
    "test_data = test_dataset\n",
    "\n",
    "test_data = pd.merge(test_data, code_act, left_on=\"act\", right_on=\"Code\", how=\"left\")\n",
    "test_data.drop([\"Code\", \"Libellé\"], axis=1, inplace=True)\n",
    "test_data = pd.merge(test_data, code_HIGHEST_CREDENTIAL, left_on=\"HIGHEST_CREDENTIAL\", right_on=\"Code\", how=\"left\")\n",
    "test_data.drop([\"Code\", \"Libellé\", \"HIGHEST_CREDENTIAL\"], axis=1, inplace=True)\n",
    "\n",
    "test_data = pd.merge(test_data, city_pop, on=\"INSEE_CODE\", how=\"left\")\n",
    "test_data = pd.merge(test_data, city_loc, on=\"INSEE_CODE\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_dfs = [learn_dataset_emp_contract, learn_dataset_job, learn_dataset_retired_former, \n",
    "             learn_dataset_retired_jobs, learn_dataset_retired_pension, learn_sports]\n",
    "\n",
    "test_dfs = [test_dataset_emp_contract, test_dataset_job, test_dataset_retired_former, \n",
    "            test_dataset_retired_jobs, test_dataset_retired_pension, test_sports]\n",
    "\n",
    "for df in learn_dfs:\n",
    "    learn_data = pd.merge(learn_data, df, on=\"PRIMARY_KEY\", how=\"outer\")\n",
    "\n",
    "for df in test_dfs:\n",
    "    test_data = pd.merge(test_data, df, on=\"PRIMARY_KEY\", how=\"outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to combine columns ending with `_x` and `_y` into a single base column\n",
    "def combine_duplicate_columns(dataframe):\n",
    "    for column in dataframe.columns:\n",
    "        if column.endswith('_x'):\n",
    "            base_column = column[:-2]  # Remove `_x` suffix\n",
    "            y_column = base_column + '_y'\n",
    "            if y_column in dataframe.columns:\n",
    "                # Combine the `_x` and `_y` columns\n",
    "                dataframe[base_column] = dataframe[column].fillna(dataframe[y_column])\n",
    "                # Drop the original `_x` and `_y` columns\n",
    "                dataframe.drop(columns=[column, y_column], inplace=True)\n",
    "    return dataframe\n",
    "\n",
    "# Apply the function to both datasets\n",
    "learn_data = combine_duplicate_columns(learn_data)\n",
    "test_data = combine_duplicate_columns(test_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formatting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def household_num(value):\n",
    "    parts = value.split('|')  # Split the value by '|'\n",
    "    if parts[1] in {'1', '2', '3'}:  # For M|1|-- to M|3|--\n",
    "        return int(parts[1])\n",
    "    elif parts[1] == '4':  # For M|4|1 to M|4|4\n",
    "        return 4 + (int(parts[2]) - 1)  # 4 + (1-1), 4 + (2-1), etc.\n",
    "    return None  # Handle unexpected cases gracefully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_HOUSEHOLD_TYPE['HOUSEHOLD_TYPE_num'] = code_HOUSEHOLD_TYPE['Code'].apply(household_num)\n",
    "learn_data['HOUSEHOLD_TYPE'] = learn_data['HOUSEHOLD_TYPE'].apply(household_num)\n",
    "test_data['HOUSEHOLD_TYPE'] = test_data['HOUSEHOLD_TYPE'].apply(household_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_columns(primary_col, fallback_col):\n",
    "    \"\"\"Combine two columns, filling missing values in the primary column with values from the fallback column.\"\"\"\n",
    "    return primary_col.fillna(fallback_col) if fallback_col is not None else primary_col\n",
    "\n",
    "def preprocess_employee_data(data, economic_sector_code, work_description_map):\n",
    "    # Extract numeric values from specific string columns\n",
    "    data[\"employee_count\"] = data[\"employee_count\"].str.extract(r'tr_(\\d)')[0].astype(\"Int64\")\n",
    "    data[\"Employer_category\"] = data[\"Employer_category\"].str.extract(r'ct_(\\d)')[0].astype(\"Int64\")\n",
    "    \n",
    "    # Merge with economic sector codes\n",
    "    data = data.merge(economic_sector_code, left_on=\"Economic_sector\", right_on=\"Code\", how=\"left\")\n",
    "    \n",
    "    # Merge with work description map and clean up columns\n",
    "    data = data.merge(work_description_map, left_on=\"work_description\", right_on=\"N3\", how=\"left\")\n",
    "    data.drop([\"work_description\", \"N3\", \"N2\"], axis=1, inplace=True)\n",
    "    data[\"work_description\"] = data[\"N1\"].str.extract(r'csp_(\\d)')[0].astype(\"Int64\")\n",
    "    data.drop(\"N1\", axis=1, inplace=True)\n",
    "    \n",
    "    # Combine relevant columns for contracts and pay\n",
    "    data[\"emp_contract\"] = combine_columns(data[\"emp_contract\"], data[\"former_emp_contract\"])\n",
    "    data[\"Pay\"] = combine_columns(data[\"Pay\"], data[\"RETIREMENT_PAY\"])\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Apply preprocessing to both learn and test datasets\n",
    "learn_data = preprocess_employee_data(learn_data, code_Economic_sector, code_work_description_map)\n",
    "test_data = preprocess_employee_data(test_data, code_Economic_sector, code_work_description_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_retirement_age(data):\n",
    "    # Ensure the retirement_age column is numeric\n",
    "    data['retirement_age'] = pd.to_numeric(data['retirement_age'], errors='coerce')\n",
    "    \n",
    "    # Define the bins and corresponding labels\n",
    "    bins = [0, 57, 60, 61, 63, 65, float('inf')]  # Specify edges for the ranges\n",
    "    labels = ['<57', '57-59', '60', '61-62', '63-64', '65+']  # Labels for ranges\n",
    "\n",
    "    # Categorize retirement_age into retirement_age_cat\n",
    "    data['retirement_age_cat'] = pd.cut(\n",
    "        data['retirement_age'], \n",
    "        bins=bins, \n",
    "        labels=labels, \n",
    "        right=False,  # Left-closed intervals\n",
    "        include_lowest=True\n",
    "    )\n",
    "    \n",
    "    # Ensure missing values in retirement_age_cat are handled properly\n",
    "    data['retirement_age_cat'] = data['retirement_age_cat'].astype(object)  # Avoid ambiguity with NA\n",
    "    \n",
    "    # Handle exact matches for 60 and 65\n",
    "    data.loc[data['retirement_age'] == 60, 'retirement_age_cat'] = '60'\n",
    "\n",
    "# Apply the function to both datasets\n",
    "categorize_retirement_age(learn_data)\n",
    "categorize_retirement_age(test_data)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess column types\n",
    "def preprocess_column_types(data):\n",
    "    data[\"sex\"] = pd.factorize(data[\"sex\"])[0]\n",
    "    data[\"studying\"] = data[\"studying\"].astype(\"int64\")\n",
    "    data[\"Sports_Category\"] = data[\"Sports_Category\"].fillna(0).astype(\"int64\")\n",
    "    \n",
    "    # List of columns to convert to Int64\n",
    "    int_columns = [\"JOB_REG\", \"FORMER_REG\", \"retirement_age\", \"WORKING_HOURS\", \"Economic_sector_num\", \"Pay\"]\n",
    "    \n",
    "    for col in int_columns:\n",
    "        data[col] = pd.to_numeric(data[col], errors='coerce').astype('Int64')\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Apply the function to both datasets\n",
    "learn_data = preprocess_column_types(learn_data)\n",
    "test_data = preprocess_column_types(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_na_with_category(data, column_name):\n",
    "    # Convert the column to categorical\n",
    "    data[column_name] = data[column_name].astype('category')\n",
    "    \n",
    "    # Define categories to add\n",
    "    additional_categories = ['Unemployed', 'Retired_Missing', 'Employed_Missing']\n",
    "    \n",
    "    # Add the specified categories\n",
    "    data[column_name] = data[column_name].cat.add_categories(additional_categories)\n",
    "    \n",
    "    # Assign categories based on conditions\n",
    "    data.loc[(data[column_name].isna()) & (data['JOB_42'].astype(str).str.startswith('csp_7')), column_name] = 'Retired_Missing'\n",
    "    data.loc[(data[column_name].isna()) & (data['act_num'] == 1), column_name] = 'Employed_Missing'\n",
    "    data.loc[(data[column_name].isna()) & ((data['JOB_42'].astype(str).str.startswith('csp_8')) | (data['act_num'] == 2)), column_name] = 'Unemployed'\n",
    "\n",
    "# List of columns to process\n",
    "columns_to_process = [\n",
    "    \"emp_contract\", \"TYPE_OF_CONTRACT\", \"WORK_CONDITION\", \n",
    "    \"labor_force_status\", \"Economic_sector_num\", \"REG_JOB\", \n",
    "    \"REG_FORMER\", \"work_description\", \"retirement_age_cat\"\n",
    "]\n",
    "\n",
    "# Apply the function to each column for both datasets\n",
    "for column in columns_to_process:\n",
    "    replace_na_with_category(learn_data, column)\n",
    "    replace_na_with_category(test_data, column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_values(data, column, unemployed_value, default_value):\n",
    "    data[column] = data.apply(\n",
    "        lambda row: unemployed_value if pd.isna(row[column]) and \n",
    "                        (str(row['JOB_42']).startswith('csp_8') or row['act_num'] == 2)  # unemployed\n",
    "                    else (default_value if pd.isna(row[column]) else row[column]),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "# Fill missing values for both datasets\n",
    "fill_missing_values(learn_data, 'Employer_category', unemployed_value=10, default_value=0)\n",
    "fill_missing_values(learn_data, 'employee_count', unemployed_value=7, default_value=0)\n",
    "\n",
    "fill_missing_values(test_data, 'Employer_category', unemployed_value=10, default_value=0)\n",
    "fill_missing_values(test_data, 'employee_count', unemployed_value=7, default_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(data, drop_columns):\n",
    "    # Replace JOB_42 with FORMER_JOB_42 where applicable\n",
    "    data.loc[data['JOB_42'].astype(str).str.startswith('csp_7'), 'JOB_42'] = data['FORMER_JOB_42']\n",
    "    \n",
    "    # Fill missing Pay and WORKING_HOURS for unemployed\n",
    "    data.loc[(data['emp_contract'] == 'Unemployed') & (data['Pay'].isna()), 'Pay'] = 0\n",
    "    data.loc[(data['emp_contract'] == 'Unemployed') & (data['WORKING_HOURS'].isna()), 'WORKING_HOURS'] = 0\n",
    "    \n",
    "    # Drop unnecessary columns\n",
    "    return data.drop(columns=drop_columns)\n",
    "\n",
    "# Define columns to drop for each dataset\n",
    "drop_columns = [\n",
    "    \"act\", \"former_emp_contract\", \"RETIREMENT_PAY\", \"retirement_age\", \n",
    "    \"FORMER_JOB_42\", \"Economic_sector\", \"Code\", \"Libellé\", \n",
    "    \"Nomenclature\", \"X\", \"Y\", \"INSEE_CODE\"\n",
    "]\n",
    "\n",
    "# Apply the cleaning function\n",
    "learn_data = clean_data(learn_data, drop_columns)\n",
    "test_data = clean_data(test_data, drop_columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_values_table(df):\n",
    "        mis_val = df.isnull().sum()\n",
    "        mis_val_percent = 100 * df.isnull().sum() / len(df)  #% of missing values\n",
    "        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)  #create result table\n",
    "        mis_val_table_ren_columns = mis_val_table.rename(\n",
    "        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n",
    "        mis_val_table_ren_columns = mis_val_table_ren_columns[\n",
    "            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n",
    "        '% of Total Values', ascending=False).round(1)\n",
    "        print (\"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n",
    "              \" columns that have missing values.\")\n",
    "        return mis_val_table_ren_columns\n",
    "\n",
    "print(missing_values_table(learn_data)) \n",
    "print(missing_values_table(test_data)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replacing missing values with mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replacement with values using external data for pay and working hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define job-specific pay and working hours\n",
    "job_defaults = {\n",
    "    'csp_1': {'Pay': 50000, 'WORKING_HOURS': 2860},  # Agriculture\n",
    "    'csp_2_1': {'Pay': 24000, 'WORKING_HOURS': 2288},  # Artisans\n",
    "    'csp_2_2': {'Pay': 39937, 'WORKING_HOURS': 2444},  # Commerçant\n",
    "    'csp_2_3': {'Pay': 58248, 'WORKING_HOURS': 2704},  # Chefs d'enterprise\n",
    "}\n",
    "\n",
    "# Function to fill missing values for Pay and WORKING_HOURS\n",
    "def fill_job_defaults(data, job_defaults):\n",
    "    for job, defaults in job_defaults.items():\n",
    "        if job == 'csp_1':\n",
    "            # Special case: Handle 'startswith' condition for 'csp_1'\n",
    "            job_condition = data['JOB_42'].astype(str).str.startswith(job)\n",
    "        else:\n",
    "            # Exact match for other job keys\n",
    "            job_condition = data['JOB_42'] == job\n",
    "        \n",
    "        # Fill missing Pay and WORKING_HOURS\n",
    "        data.loc[job_condition & data['Pay'].isna(), 'Pay'] = defaults['Pay']\n",
    "        data.loc[job_condition & data['WORKING_HOURS'].isna(), 'WORKING_HOURS'] = defaults['WORKING_HOURS']\n",
    "\n",
    "# Apply the function to the datasets\n",
    "fill_job_defaults(learn_data, job_defaults)\n",
    "fill_job_defaults(test_data, job_defaults)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values_learn_data = missing_values_table(learn_data)\n",
    "missing_values_test_data = missing_values_table(test_data)\n",
    "\n",
    "# Convert the missing values table to LaTeX format\n",
    "latex_learn_data = missing_values_learn_data.to_latex(index=False, float_format=\"%.2f\")\n",
    "latex_test_data = missing_values_test_data.to_latex(index=False, float_format=\"%.2f\")\n",
    "\n",
    "with open(\"./missing_values_learn_data.tex\", \"w\") as file:\n",
    "    file.write(latex_learn_data)\n",
    "with open(\"./missing_values_test_data.tex\", \"w\") as file:\n",
    "    file.write(latex_test_data)\n",
    "\n",
    "print(missing_values_table(learn_data)) \n",
    "print(missing_values_table(test_data)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sorting for numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to factorize\n",
    "columns_to_factorize = [  \n",
    "    'JOB_42', 'REG_JOB', 'Economic_sector_num', 'work_description', \n",
    "    'retirement_age_cat', 'REG_FORMER', 'TYPE_OF_CONTRACT', \n",
    "    'WORK_CONDITION', 'labor_force_status', 'emp_contract'\n",
    "]  \n",
    "\n",
    "# Function to sort and factorize columns\n",
    "def sort_and_factorize(data, columns):\n",
    "    for column in columns:\n",
    "        data = data.sort_values(by=column, ascending=True)\n",
    "        data[column] = pd.factorize(data[column])[0]\n",
    "    return data.sort_values(by='PRIMARY_KEY', ascending=True)\n",
    "\n",
    "# Apply the function to learn_data and test_data\n",
    "learn_data = sort_and_factorize(learn_data, columns_to_factorize)\n",
    "test_data = sort_and_factorize(test_data, columns_to_factorize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Transformer for Handling Imputation with Averages\n",
    "class ImputeMissingValues(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        # Calculate averages for non-missing, non-zero values\n",
    "        self.avg_working_hours = (\n",
    "            X.loc[X[\"WORKING_HOURS\"].notna() & (X[\"WORKING_HOURS\"] != 0)]\n",
    "            .groupby(\"JOB_42\")[\"WORKING_HOURS\"]\n",
    "            .mean()\n",
    "            .round()\n",
    "        )\n",
    "        self.avg_pay = (\n",
    "            X.loc[X[\"Pay\"].notna() & (X[\"Pay\"] != 0)]\n",
    "            .groupby(\"JOB_42\")[\"Pay\"]\n",
    "            .mean()\n",
    "            .round()\n",
    "        )\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        # Make a copy of X to avoid modifying the original dataframe\n",
    "        X = X.copy()\n",
    "        \n",
    "        # Impute WORKING_HOURS using averages\n",
    "        X[\"WORKING_HOURS\"] = X.apply(\n",
    "            lambda row: self.avg_working_hours.get(row[\"JOB_42\"], np.nan)\n",
    "            if pd.isnull(row[\"WORKING_HOURS\"]) else row[\"WORKING_HOURS\"],\n",
    "            axis=1\n",
    "        )\n",
    "        \n",
    "        # Impute Pay using averages\n",
    "        X[\"Pay\"] = X.apply(\n",
    "            lambda row: self.avg_pay.get(row[\"JOB_42\"], np.nan)\n",
    "            if pd.isnull(row[\"Pay\"]) else row[\"Pay\"],\n",
    "            axis=1\n",
    "        )\n",
    "        \n",
    "        return X\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Transformer for MICE (Multiple Imputation by Chained Equations) Imputation\n",
    "class MICEImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns_to_impute):\n",
    "        self.columns_to_impute = columns_to_impute\n",
    "        self.imputer = IterativeImputer(random_state=100)\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        # Fit the imputer only on the specified columns\n",
    "        self.imputer.fit(X[self.columns_to_impute])\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # Transform only the specified columns\n",
    "        X_imputed = X.copy()\n",
    "        X_imputed[self.columns_to_impute] = self.imputer.transform(X[self.columns_to_impute])\n",
    "        return X_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the columns that need MICE imputation\n",
    "columns_to_impute = [\"Employer_category\", \"employee_count\"]\n",
    "\n",
    "# Define the combined pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('impute_averages', ImputeMissingValues()),  # Step 1: Impute using averages\n",
    "    ('mice_imputer', MICEImputer(columns_to_impute=columns_to_impute)),  # Step 2: MICE imputation\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for linear regression I need only number \n",
    "\n",
    "#do one-hot encoding for \"labor_force status\", but with catagegory being 0, Employed_missing, Unemployed and Other (=only being 6%)\n",
    "#do one hot-encodung for \"type_of_contratct\" with CDI, Unemployed, Employed_Missing, Other (=only being 12%)\n",
    "#do one_hot_encding for \"work_condition\" with Unemployed, P, Other(=only being 17%)\n",
    "#do one hot_econding for \"emp_contract\", with EMP1-6, Unemployed, EMP2-1, Other (=being 12%)\n",
    "#can keep as is for \"highest_credential\" because the ordinality make sense here for education\n",
    "#same for the location of insee code \n",
    "#use \"work condition\" instead of JOB_42 and then do one-hot-encdoding\n",
    "#do one-hot-encoding for \"household_type\"\n",
    "#for \"act\" do one hot encoding but maybe merge stay at home people with inactif + drop less than 14years olds as nobody in our data is \n",
    "#do one hot encoding for \"sport\" but with a \"Other\" category\n",
    "#do one hot encdoding for \"REG_JOB\" and \"FORMER_REG\"\n",
    "#do one hot encdoding for \"employer_category\" with 1 to 7 being \"Others\n",
    "#can keep it as is for \"employee count\" because here the ordinality make sense\n",
    "#do one hot encoding for \"Economic_sector_num\" and \"work_description\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_lin = pipeline.fit_transform(learn_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check\n",
    "def missing_values_table(df):\n",
    "        mis_val = df.isnull().sum()\n",
    "        mis_val_percent = 100 * df.isnull().sum() / len(df)  #% of missing values\n",
    "        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)  #create result table\n",
    "        mis_val_table_ren_columns = mis_val_table.rename(\n",
    "        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n",
    "        mis_val_table_ren_columns = mis_val_table_ren_columns[\n",
    "            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n",
    "        '% of Total Values', ascending=False).round(1)\n",
    "        print (\"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n",
    "              \" columns that have missing values.\")\n",
    "        return mis_val_table_ren_columns\n",
    "\n",
    "print(missing_values_table(learn_lin)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this gives 0 and 1\n",
    "def preprocess_for_regression(data):\n",
    "    # Initialize OneHotEncoder with sparse output disabled to return dense arrays\n",
    "    ohe = OneHotEncoder(sparse_output=False, dtype=int, handle_unknown='ignore')\n",
    "    \n",
    "    # List of columns to encode\n",
    "    categorical_columns = [\n",
    "        'HOUSEHOLD_TYPE', 'act_num', 'Sports_Category', \n",
    "        'Employer_category', \"employee_count\",\n",
    "        'JOB_42', 'REG_JOB', 'Economic_sector_num', \n",
    "        'work_description', 'REG_FORMER', 'TYPE_OF_CONTRACT',\n",
    "        'WORK_CONDITION', 'labor_force_status', 'emp_contract'\n",
    "    ]\n",
    "\n",
    "    # List of columns to encode categorical_columns = [\n",
    "        #'labor_force_status', 'TYPE_OF_CONTRACT', 'WORK_CONDITION', \n",
    "        #'emp_contract', 'HOUSEHOLD_TYPE', 'act_num', 'Sports_Category', \n",
    "        #'REG_JOB', 'REG_FORMER', 'Employer_category', 'Economic_sector_num', \n",
    "        #'work_description' ]\n",
    "    \n",
    "    # Apply the encoder\n",
    "    encoded_data = ohe.fit_transform(data[categorical_columns])\n",
    "    \n",
    "    # Create a DataFrame for the encoded features\n",
    "    encoded_feature_names = ohe.get_feature_names_out(categorical_columns)\n",
    "    encoded_df = pd.DataFrame(encoded_data, columns=encoded_feature_names, index=data.index)\n",
    "    \n",
    "    # Drop original categorical columns from the dataset\n",
    "    data = data.drop(columns=categorical_columns)\n",
    "    \n",
    "    # Merge encoded features with the original dataset\n",
    "    processed_data = pd.concat([data, encoded_df], axis=1)\n",
    "    \n",
    "    return processed_data\n",
    "\n",
    "\n",
    "# Apply preprocessing to learn and test datasets\n",
    "learn_lin = preprocess_for_regression(learn_lin)\n",
    "#test_data = preprocess_for_regression(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_lin = learn_lin.drop(columns=[\"target\"])\n",
    "y_train_lin = learn_lin[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_lin, X_test_lin, Y_train_lin, Y_test_lin = train_test_split(x_train_lin,\n",
    "                                                    y_train_lin, \n",
    "                                                    test_size=0.3, \n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example GridSearchCV for Linear Regression\n",
    "param_grid = {'fit_intercept': [True, False]}  # Define your hyperparameter grid\n",
    "\n",
    "# Create a Linear Regression model\n",
    "lr = LinearRegression()\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "lin_res = GridSearchCV(estimator=lr, param_grid=param_grid, cv=5)\n",
    "\n",
    "# Fit the GridSearchCV model\n",
    "lin_res.fit(X_train_lin, Y_train_lin)\n",
    "\n",
    "# Output the best model and its score\n",
    "print(f\"Best model: {lin_res.best_estimator_}\")\n",
    "print(f\"Best score: {lin_res.best_score_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions = lin_res.predict(X_train_lin)\n",
    "print(\"R² for training set: \", r2_score(Y_train_lin, train_predictions))\n",
    "print(\"RMSE on the learning set:\", root_mean_squared_error(Y_train_lin, lin_res.predict(X_train_lin)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions = lin_res.predict(X_train_lin)\n",
    "test_predictions = lin_res.predict(X_test_lin)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x=Y_test_lin, y=test_predictions, alpha=0.6, color=\"blue\")\n",
    "plt.plot([y_train_lin.min(), y_train_lin.max()], [y_train_lin.min(), y_train_lin.max()], \"--r\", linewidth=2)  # Ideal predictions\n",
    "plt.xlabel('True Target Value')\n",
    "plt.ylabel('Predicted Target Value')\n",
    "plt.title('Prediction vs True Values on Test Set')\n",
    "plt.savefig(\"./Prediction_vs_True_linear.png\", format='png', dpi=300)  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_rf = pipeline.fit_transform(learn_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_rf = learn_rf.drop(columns=[\"target\"])\n",
    "y_train_rf = learn_rf[\"target\"]\n",
    "\n",
    "X_train_rf, X_test_rf, Y_train_rf, Y_test_rf = train_test_split(x_train_rf,\n",
    "                                                    y_train_rf, \n",
    "                                                    test_size=0.3, \n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_rf = {     # Number of trees in the forest\n",
    "    'max_depth': [5, 10, 30, 50],    \n",
    "    'min_samples_split': [5, 10, 20]  \n",
    "}\n",
    "\n",
    "# Define cross-validation with 5 folds\n",
    "cv_folds = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize the RandomForestRegressor model\n",
    "rf = RandomForestRegressor(random_state=42)  #need to add that in the pipeline \n",
    "\n",
    "# Set up the GridSearchCV for RandomForestRegressor with appropriate scoring metric\n",
    "rf_search = GridSearchCV(rf, param_grid_rf, cv=cv_folds, n_jobs=-1, scoring=\"neg_mean_squared_error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model on the training data\n",
    "rf_res = rf_search.fit(X_train_rf, Y_train_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best model after GridSearchCV\n",
    "best_model_rf = rf_res.best_estimator_\n",
    "\n",
    "# Output the best model and its score\n",
    "print(f\"Best model: {best_model_rf}\")\n",
    "print(f\"Best score: {rf_res.best_score_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_scores = cross_val_score(best_model_rf, X_train_rf, Y_train_rf, cv=5, scoring='r2')\n",
    "print(f\"Cross-validated R2 scores: {cv_scores}\")\n",
    "print(f\"Average R2: {np.mean(cv_scores):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions = best_model_rf.predict(X_train_rf)\n",
    "test_predictions = best_model_rf.predict(X_test_rf)\n",
    "\n",
    "print(\"R² for training set: \", r2_score(Y_train_rf, train_predictions))\n",
    "print(\"R² for test set: \", r2_score(Y_test_rf, test_predictions))\n",
    "print(\"RMSE on the learning set:\", root_mean_squared_error(Y_train_rf, best_model_rf.predict(X_train_rf)))\n",
    "print(\"RMSE on the test set:\", root_mean_squared_error(Y_test_rf, best_model_rf.predict(X_test_rf)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_metrics = pd.DataFrame({\n",
    "    'Metric': ['R²', 'RMSE'],\n",
    "    'Train': [r2_score(Y_train_rf, train_predictions), root_mean_squared_error(Y_train_rf, best_model_rf.predict(X_train_rf))],\n",
    "    'Test': [r2_score(Y_test_rf, test_predictions), root_mean_squared_error(Y_test_rf, best_model_rf.predict(X_test_rf))]\n",
    "})\n",
    "\n",
    "# Set 'Metric' as the index to have a clean format\n",
    "error_metrics.set_index('Metric', inplace=True)\n",
    "\n",
    "# Convert to LaTeX format\n",
    "error_metrics_table = error_metrics.to_latex(index=True, float_format=\"%.2f\")\n",
    "\n",
    "with open(\"./error_metrics.tex\", \"w\") as file:\n",
    "    file.write(error_metrics_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x=Y_test_rf, y=test_predictions, alpha=0.6, color=\"blue\")\n",
    "plt.plot([y_train_rf.min(), y_train_rf.max()], [y_train_rf.min(), y_train_rf.max()], \"--r\", linewidth=2)  # Ideal predictions\n",
    "plt.xlabel('True Target Value')\n",
    "plt.ylabel('Predicted Target Value')\n",
    "plt.title('Prediction vs True Values on Test Set')\n",
    "plt.savefig(\"./Prediction_vs_True_rf.png\", format='png', dpi=300)  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation\n",
    "correlation_matrix = learn_data.corr()\n",
    "\n",
    "# Find correlations with the target variable\n",
    "target_corr = correlation_matrix['target'].sort_values(ascending=False)\n",
    "\n",
    "print(target_corr)\n",
    "\n",
    "target_corr_table = target_corr.to_latex(index=False, float_format=\"%.2f\")\n",
    "\n",
    "with open(\"./target_corr.tex\", \"w\") as file:\n",
    "    file.write(target_corr_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances\n",
    "feature_importances = pd.DataFrame({\n",
    "    'Feature': x_train_rf.columns,\n",
    "    'Importance': best_model_rf.feature_importances_\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(feature_importances)\n",
    "\n",
    "feature_importances_table = feature_importances.to_latex(index=False, float_format=\"%.2f\")\n",
    "\n",
    "with open(\"./feature_importances.tex\", \"w\") as file:\n",
    "    file.write(feature_importances_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## these didn't work yet - ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = rf_res.cv_results_\n",
    "params = results['params']\n",
    "\n",
    "# Extract individual fold scores\n",
    "fold_scores = []\n",
    "for i in range(10):\n",
    "    fold_scores.append(-results[f'split{i}_test_score'])  # Negate for RMSE\n",
    "\n",
    "\n",
    "# Create a DataFrame to store the results, including fold information\n",
    "df = pd.DataFrame(params)\n",
    "for i, fold_score in enumerate(fold_scores):\n",
    "    df[f'fold_{i}'] = fold_score\n",
    "\n",
    "# Melt the DataFrame to long format for easier plotting\n",
    "df_melted = df.melt(id_vars=['max_depth', 'min_samples_split'], var_name='fold', value_name='rmse')\n",
    "\n",
    "# Create a line plot for RMSE as a function of min_samples_split, with different colors for each max_depth\n",
    "sns.lineplot(x='min_samples_split', y='rmse', hue='max_depth', data=df_melted, errorbar='se')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_folds = StratifiedKFold(10, shuffle=True, random_state=0)\n",
    "dt = DecisionTreeRegressor(random_state=0)\n",
    "dt_grid = {'min_samples_split': [2, 5] + list(range(10, 250,5)),\n",
    "           'max_depth': [2, 5, 10, 15, 20]}\n",
    "\n",
    "rmse_train = []\n",
    "rmse_test = []\n",
    "rmse_cv = []\n",
    "params_cv = []\n",
    "for k in range(50):\n",
    "    ## we use k for the random state to introduce different CV splits\n",
    "    cv_folds = StratifiedKFold(10, shuffle=True, random_state=k)\n",
    "    dt = RandomForestRegressor(random_state=0)\n",
    "    dt_cv = GridSearchCV(dt, dt_grid, cv=cv_folds, n_jobs=-1, \n",
    "                     scoring='neg_root_mean_squared_error')\n",
    "    dt_cv.fit(X_train, Y_train)\n",
    "    rmse_cv.append(-dt_cv.best_score_)\n",
    "    rmse_train.append(root_mean_squared_error(Y_train, dt_cv.predict(X_train)))\n",
    "    rmse_test.append(root_mean_squared_error(Y_test, dt_cv.predict(X_test)))\n",
    "    params_cv.append(dt_cv.best_params_)\n",
    "    \n",
    "#%% \n",
    "all_results = pd.DataFrame({'run': list(range(len(rmse_cv))),\n",
    "                            'CV': rmse_cv,\n",
    "                            'train': rmse_train,\n",
    "                            'test': rmse_test\n",
    "    })\n",
    "\n",
    "res = all_results.melt('run',var_name='Estimate',value_name='Accuracy')\n",
    "sns.displot(data=res,x='Accuracy',kind='kde',hue='Estimate',fill=True)\n",
    "\n",
    "param_results = pd.DataFrame({'run': list(range(len(rmse_cv))),\n",
    "                              'max_depth': [x['max_depth'] for x in params_cv],\n",
    "                              'min_samples_split': [x['min_samples_split'] for x in params_cv]\n",
    "                              })\n",
    "\n",
    "sns.displot(data=param_results,x='max_depth')\n",
    "\n",
    "sns.displot(data=param_results,x='min_samples_split')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## DO NOT RUN - just look at output lol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample dataset (replace this with your own dataset)\n",
    "# X = features, y = target variable\n",
    "# X, y = your_data.drop(\"target\", axis=1), your_data[\"target\"]\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "nX_train, nX_test, ny_train, ny_test = train_test_split(x_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "pipeline.fit_transform(nX_train, ny_train)\n",
    "\n",
    "# Random Forest Regressor\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# 1. Fit the model\n",
    "rf.fit(nX_train, ny_train)\n",
    "\n",
    "# 2. Predictions\n",
    "ny_pred = rf.predict(nX_test)\n",
    "\n",
    "# 3. Performance Metrics\n",
    "mse = mean_squared_error(ny_test, ny_pred)\n",
    "r2 = r2_score(ny_test, ny_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse:.2f}\")\n",
    "print(f\"R-squared: {r2:.2f}\")\n",
    "\n",
    "# Graphical Visualization of Predictions\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(ny_test, ny_pred, alpha=0.7, color=\"blue\")\n",
    "plt.plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], \"--r\", linewidth=2)  # Ideal predictions\n",
    "plt.title(\"Actual vs Predicted Values\")\n",
    "plt.xlabel(\"Actual Values\")\n",
    "plt.ylabel(\"Predicted Values\")\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# 4. Resampling using cross-validation\n",
    "cv_scores = cross_val_score(rf, x_train, y_train, cv=5, scoring='r2')\n",
    "print(f\"Cross-validated R2 scores: {cv_scores}\")\n",
    "print(f\"Average R2: {np.mean(cv_scores):.2f}\")\n",
    "\n",
    "# 5. Hyperparameter Tuning with Grid Search\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, \n",
    "                           cv=5, n_jobs=-1, scoring='r2', verbose=2)\n",
    "grid_search.fit(nX_train, ny_train)\n",
    "\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "\n",
    "# Evaluate the best model\n",
    "best_rf = grid_search.best_estimator_\n",
    "print(f\"Best model: {best_rf}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best Parameters: {'bootstrap': True, 'max_depth': 30, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 300}\n",
    "\n",
    "pipeline.transform(nX_test)\n",
    "\n",
    "y_pred_best = best_rf.predict(nX_test)\n",
    "\n",
    "# Graphical Visualization of Best Model Predictions\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(ny_test - y_pred_best, kde=True, bins=30, color=\"green\")\n",
    "plt.title(\"Residual Distribution of Best Model\")\n",
    "plt.xlabel(\"Residuals (y_test - y_pred_best)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rf = pipeline.transform(test_data)\n",
    "test_predictions = best_model_rf.predict(test_rf)  # Replace with your model's prediction method\n",
    "\n",
    "predictions_df = pd.DataFrame({\n",
    "    'PRIMARY_KEY': test_rf['PRIMARY_KEY'], \n",
    "    'target': test_predictions\n",
    "})\n",
    "\n",
    "# Save the predictions to a CSV file\n",
    "predictions_df.to_csv('predictions.csv', index=False, sep=',', float_format='%.6f')  # Save with US decimal notation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.read_csv(\"predictions.csv\")\n",
    "predictions\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
